{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a1f10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Mkt_Cap</th>\n",
       "      <th>Mkt_Cap_SOM</th>\n",
       "      <th>TR</th>\n",
       "      <th>NTM_RevGrowth_EOM</th>\n",
       "      <th>2y_RevGrowth_EOM</th>\n",
       "      <th>NTM_RevGrowth_3mChg_SOM</th>\n",
       "      <th>NTM_Rev_3mChg_SOM</th>\n",
       "      <th>NTM_EBITDA_Margin_EOM</th>\n",
       "      <th>...</th>\n",
       "      <th>PB_TTM</th>\n",
       "      <th>BY_1y_Fwd</th>\n",
       "      <th>PB_1y_Fwd</th>\n",
       "      <th>BY_2y_Fwd</th>\n",
       "      <th>PB_2y_Fwd</th>\n",
       "      <th>DY_TTM</th>\n",
       "      <th>DY_1y_Fwd</th>\n",
       "      <th>DY_2y_Fwd</th>\n",
       "      <th>NOSH_Chg_LTM</th>\n",
       "      <th>TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1989-09-30</td>\n",
       "      <td>US0318971019</td>\n",
       "      <td>4888.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1989-09-30</td>\n",
       "      <td>US0017651060</td>\n",
       "      <td>5101.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41614.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1989-09-30</td>\n",
       "      <td>US0434131035</td>\n",
       "      <td>1432.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1989-09-30</td>\n",
       "      <td>US0019575051</td>\n",
       "      <td>47338.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38674.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1989-09-30</td>\n",
       "      <td>US0028241000</td>\n",
       "      <td>14258.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45580.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date          ISIN   Mkt_Cap  Mkt_Cap_SOM  TR  NTM_RevGrowth_EOM  \\\n",
       "0 1989-09-30  US0318971019   4888.74          NaN NaN                NaN   \n",
       "1 1989-09-30  US0017651060   5101.33          NaN NaN                NaN   \n",
       "2 1989-09-30  US0434131035   1432.45          NaN NaN                NaN   \n",
       "3 1989-09-30  US0019575051  47338.82          NaN NaN                NaN   \n",
       "4 1989-09-30  US0028241000  14258.13          NaN NaN                NaN   \n",
       "\n",
       "   2y_RevGrowth_EOM  NTM_RevGrowth_3mChg_SOM  NTM_Rev_3mChg_SOM  \\\n",
       "0               NaN                      NaN                NaN   \n",
       "1               NaN                      NaN                NaN   \n",
       "2               NaN                      NaN                NaN   \n",
       "3               NaN                      NaN                NaN   \n",
       "4               NaN                      NaN                NaN   \n",
       "\n",
       "   NTM_EBITDA_Margin_EOM  ...  PB_TTM  BY_1y_Fwd  PB_1y_Fwd  BY_2y_Fwd  \\\n",
       "0                    NaN  ...     NaN        NaN        NaN        NaN   \n",
       "1                    NaN  ...     NaN        NaN        NaN        NaN   \n",
       "2                    NaN  ...     NaN        NaN        NaN        NaN   \n",
       "3                    NaN  ...     NaN        NaN        NaN        NaN   \n",
       "4                    NaN  ...     NaN        NaN        NaN        NaN   \n",
       "\n",
       "   PB_2y_Fwd  DY_TTM  DY_1y_Fwd  DY_2y_Fwd  NOSH_Chg_LTM     TIME  \n",
       "0        NaN     NaN        NaN        NaN           NaN  36251.0  \n",
       "1        NaN     NaN        NaN        NaN           NaN  41614.0  \n",
       "2        NaN     NaN        NaN        NaN           NaN  36481.0  \n",
       "3        NaN     NaN        NaN        NaN           NaN  38674.0  \n",
       "4        NaN     NaN        NaN        NaN           NaN  45580.0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "## Define file path depending on which index to examine\n",
    "\n",
    "## S&P500\n",
    "index_file_path = \"H:/Tech Hardware Shared/$Mike/Quant/SnP500_Constit.csv\"\n",
    "SOX_file_path = \"H:/Tech Hardware Shared/$Mike/Quant/SOX_Constit.csv\"\n",
    "csv_directory = \"H:/Tech Hardware Shared/$Mike/Quant/CSV_files\"\n",
    "export_path = \"H:/Tech Hardware Shared/$Mike/Quant/Python_Outputs/\"\n",
    "export_audit_file_name = 'SnP500_ex_SOX_audit.xlsx'\n",
    "export_index_file_name = 'SnP500_ex_SOX_Index_Avgs.xlsx'\n",
    "export_LS_file_name = 'SnP500_ex_SOX_LS.xlsx'\n",
    "export_dispersion_file_name = 'SnP500_ex_SOX_dispersion.xlsx'\n",
    "export_examine_CSV_file_name = 'SnP500_ex_SOX_indiv_CSV.xlsx'\n",
    "\n",
    "##### CREATING DATABASE DATAFRAME ######\n",
    "\n",
    "# Load index file\n",
    "df_index = pd.read_csv(index_file_path, header=0)\n",
    "df_index.columns = pd.to_datetime(df_index.columns, format='%d-%b-%y')\n",
    "df_index = pd.melt(df_index, id_vars=[], var_name='Date', value_name='ISIN')\n",
    "df_index['Date'] = df_index['Date'] + pd.offsets.MonthEnd(0)\n",
    "df_index['ISIN'].fillna('placeholder', inplace=True)\n",
    "\n",
    "# Load SOX\n",
    "df_SOX = pd.read_csv(SOX_file_path, header=0)\n",
    "df_SOX.columns = pd.to_datetime(df_SOX.columns, format='%d-%b-%y')\n",
    "df_SOX = pd.melt(df_SOX, id_vars=[], var_name='Date', value_name='ISIN')\n",
    "df_SOX['Date'] = df_SOX['Date'] + pd.offsets.MonthEnd(0)\n",
    "df_SOX['ISIN'].fillna('placeholder', inplace=True)\n",
    "\n",
    "# Create index file excluding SOX constituents\n",
    "# performs left join, so keeps all rows from df_index and only the matching rows from df_SOX, based on columns 'Date' and 'ISIN'\n",
    "# if there are rows in df_index that don't have matches in df_SOX, those rows will appear in result, but unmatched rows from df_SOX will have NaN values\n",
    "# indicator = True adds a special column called _merge to the resulting dataframe, wh indicates the source of each row\n",
    "# 'left_only': row is presenonly in df_index; 'right_only': row is is present only in df_SOX; 'both': row is present in both\n",
    "# subsequent line filters the dataframe to keep only those rows where the _merge column is 'left_only' - ie only keeps rows from df_finces that DID NOT have a match in df_SOX\n",
    "# after filtering, the code drops the _merge column since it is no longer needed\n",
    "df = pd.merge(df_index, df_SOX, on=['Date', 'ISIN'], how='left', indicator=True)\n",
    "df = df[df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "# Create a list to store each ISIN's data\n",
    "all_isin_data = []\n",
    "\n",
    "# Iterate through each ISIN in the 'ISIN' column \n",
    "for isin in df['ISIN'].unique():\n",
    "    # Skip if the ISIN is 'placeholder'\n",
    "    if isin != 'placeholder':\n",
    "        # Construct the file path for the CSV file\n",
    "        index_file_path = os.path.join(csv_directory, f'{isin}.csv')\n",
    "        # Check if the CSV file exists\n",
    "        if os.path.exists(index_file_path):        \n",
    "            # Read the CSV file\n",
    "            isin_data = pd.read_csv(index_file_path, header=0)  # Assuming header is in row 2 and 'Date' is the label\n",
    "            isin_data['Date'] = pd.to_datetime(isin_data['Date'], unit='D', origin='1899-12-30')\n",
    "            isin_data['Date'] = isin_data['Date'] + pd.offsets.MonthEnd(0)\n",
    "            isin_data['ISIN'] = isin\n",
    "            # Convert all data columns to numeric\n",
    "                                  \n",
    "            numeric_columns = ['Mkt_Cap','TR', 'PCH',\n",
    "                               'NTM_RevGrowth','2y_RevGrowth',\n",
    "                               'NTM_EBITDA_Margin','2y_EBITDA_Margin',\n",
    "                               'RoE_TTM', 'RoE_1y_Fwd', 'RoE_2y_Fwd',\n",
    "                               'CoE', 'Assumed_G', \n",
    "                               'Sales_EV_1y_Fwd', 'EV_Sales_1y_Fwd', 'EBITDA_EV_1y_Fwd','EV_EBITDA_1y_Fwd',\n",
    "                               'EY_TTM','PE_TTM', 'EY_1y_Fwd','PE_1y_Fwd','EY_2y_Fwd','PE_2y_Fwd',\n",
    "                               'BY_TTM','PB_TTM', 'BY_1y_Fwd','PB_1y_Fwd','BY_2y_Fwd','PB_2y_Fwd',\n",
    "                               'DY_TTM','DY_1y_Fwd','DY_2y_Fwd',\n",
    "                               'NOSH_Chg_LTM',\n",
    "                               'TIME',\n",
    "                               'Mkt_Cap_SOM','TR_LTM_SOM','TR_L6M_SOM','TR_L3M_SOM',\n",
    "                               'NTM_RevGrowth_SOM','NTM_EBITDA_Margin_SOM','NTM_RoE_SOM',\n",
    "                               'NTM_RevGrowth_3mChg_SOM','NTM_Rev_3mChg_SOM',\n",
    "                               'NTM_EBITDA_Margin_3mChg_SOM',\n",
    "                               'Sales_EV_1y_Fwd_SOM','EBITDA_EV_1y_Fwd_SOM','EY_1y_Fwd_SOM','EY_2y_Fwd_SOM','BY_1y_Fwd_SOM','CoE_SOM']            \n",
    "            isin_data[numeric_columns] = isin_data[numeric_columns].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "            \n",
    "            # nix out negative BVPS data & other floors / ceilings\n",
    "            neg_cols = ['BY_TTM','PB_TTM', 'BY_1y_Fwd','PB_1y_Fwd','BY_2y_Fwd','PB_2y_Fwd','CoE']\n",
    "            for col in neg_cols:\n",
    "                isin_data[col] = np.where(isin_data[col] < 0, np.nan, isin_data[col])\n",
    "            \n",
    "            upper_bound = 0.5 # Max CoE = 50%\n",
    "            isin_data['CoE'] = isin_data['CoE'].clip(upper=upper_bound)\n",
    "                                   \n",
    "            lower_bound = -0.02 # Max multiple = -50x\n",
    "            upper_bound = 1 # Min multiple = 1x\n",
    "            isin_data['Sales_EV_1y_Fwd'] = isin_data['Sales_EV_1y_Fwd'].clip(upper=upper_bound,lower=lower_bound)\n",
    "            isin_data['EBITDA_EV_1y_Fwd'] = isin_data['EBITDA_EV_1y_Fwd'].clip(upper=upper_bound,lower=lower_bound)\n",
    "            isin_data['EY_TTM'] = isin_data['EY_TTM'].clip(upper=upper_bound,lower=lower_bound)\n",
    "            isin_data['EY_1y_Fwd'] = isin_data['EY_1y_Fwd'].clip(upper=upper_bound,lower=lower_bound)\n",
    "            isin_data['EY_2y_Fwd'] = isin_data['EY_2y_Fwd'].clip(upper=upper_bound,lower=lower_bound)\n",
    "            \n",
    "            lower_bound = -0.5\n",
    "            upper_bound = 1.5\n",
    "            isin_data['RoE_TTM'] = isin_data['RoE_TTM'].clip(upper=upper_bound,lower=lower_bound)\n",
    "            isin_data['RoE_1y_Fwd'] = isin_data['RoE_1y_Fwd'].clip(upper=upper_bound,lower=lower_bound)\n",
    "            isin_data['RoE_2y_Fwd'] = isin_data['RoE_2y_Fwd'].clip(upper=upper_bound,lower=lower_bound)\n",
    "                                        \n",
    "            # Append the data to the list\n",
    "            all_isin_data.append(isin_data)\n",
    "\n",
    "# Concatenate all ISIN data into a single DataFrame\n",
    "isin_final_df = pd.concat(all_isin_data, ignore_index=True)\n",
    "\n",
    "# Merge the additional columns into the melted DataFrame using 'Date' and 'ISIN' as the keys \n",
    "df = pd.merge(df, isin_final_df, on=['Date','ISIN'])\n",
    "\n",
    "df.head()\n",
    "\n",
    "#full_export_path = export_path + export_audit_file_name\n",
    "#df.to_excel(full_export_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971acc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CREATE DATAFRAME OF EQUAL WEIGHTED AND MARKET CAP WEIGHTED AVERAGE RATIOS ####\n",
    "\n",
    "\n",
    "### Equal Weight Calculation\n",
    "EqualWeightIndex_df = df.groupby('Date')[['TR',\n",
    "                                          'NTM_RevGrowth','2y_RevGrowth','NTM_RevGrowth_3mChg_SOM','NTM_Rev_3mChg_SOM',\n",
    "                                          'NTM_EBITDA_Margin','2y_EBITDA_Margin','NTM_EBITDA_Margin_3mChg_SOM',\n",
    "                                          'RoE_TTM', 'RoE_1y_Fwd', 'RoE_2y_Fwd',\n",
    "                                          'CoE', 'Assumed_G',\n",
    "                                          'Sales_EV_1y_Fwd','EBITDA_EV_1y_Fwd',\n",
    "                                          'EY_TTM','EY_1y_Fwd','EY_2y_Fwd',\n",
    "                                          'BY_TTM','BY_1y_Fwd','BY_2y_Fwd',\n",
    "                                          'DY_TTM','DY_1y_Fwd','DY_2y_Fwd','NOSH_Chg_LTM']].mean().reset_index()\n",
    "\n",
    "EqualWeightIndex_df.columns = [f'{col}_Eq_Wgt' if col in ['TR',\n",
    "                                          'NTM_RevGrowth','2y_RevGrowth','NTM_RevGrowth_3mChg_SOM','NTM_Rev_3mChg_SOM',\n",
    "                                          'NTM_EBITDA_Margin','2y_EBITDA_Margin','NTM_EBITDA_Margin_3mChg_SOM',\n",
    "                                          'RoE_TTM', 'RoE_1y_Fwd', 'RoE_2y_Fwd',\n",
    "                                          'CoE', 'Assumed_G',\n",
    "                                          'Sales_EV_1y_Fwd','EBITDA_EV_1y_Fwd',\n",
    "                                          'EY_TTM','EY_1y_Fwd','EY_2y_Fwd',\n",
    "                                          'BY_TTM','BY_1y_Fwd','BY_2y_Fwd',\n",
    "                                          'DY_TTM','DY_1y_Fwd','DY_2y_Fwd','NOSH_Chg_LTM'] else col for col in EqualWeightIndex_df.columns]\n",
    "\n",
    "EqualWeightIndex_df['EV_Sales_1y_Fwd_Eq_Wgt'] = 1/EqualWeightIndex_df['Sales_EV_1y_Fwd_Eq_Wgt']\n",
    "EqualWeightIndex_df['EV_EBITDA_1y_Fwd_Eq_Wgt'] = 1/EqualWeightIndex_df['EBITDA_EV_1y_Fwd_Eq_Wgt']\n",
    "EqualWeightIndex_df['PE_TTM_Eq_Wgt'] = 1/EqualWeightIndex_df['EY_TTM_Eq_Wgt']\n",
    "EqualWeightIndex_df['PE_1y_Fwd_Eq_Wgt'] = 1/EqualWeightIndex_df['EY_1y_Fwd_Eq_Wgt']\n",
    "EqualWeightIndex_df['PE_2y_Fwd_Eq_Wgt'] = 1/EqualWeightIndex_df['EY_2y_Fwd_Eq_Wgt']\n",
    "EqualWeightIndex_df['PB_TTM_Eq_Wgt'] = 1/EqualWeightIndex_df['BY_TTM_Eq_Wgt']\n",
    "EqualWeightIndex_df['PB_1y_Fwd_Eq_Wgt'] = 1/EqualWeightIndex_df['BY_1y_Fwd_Eq_Wgt']\n",
    "EqualWeightIndex_df['PB_2y_Fwd_Eq_Wgt'] = 1/EqualWeightIndex_df['BY_2y_Fwd_Eq_Wgt']\n",
    "\n",
    "\n",
    "### Median Weight Calculation\n",
    "MedianIndex_df = df.groupby('Date')[['TR',\n",
    "                                     'NTM_RevGrowth','2y_RevGrowth','NTM_RevGrowth_3mChg_SOM','NTM_Rev_3mChg_SOM',\n",
    "                                     'NTM_EBITDA_Margin','2y_EBITDA_Margin','NTM_EBITDA_Margin_3mChg_SOM',\n",
    "                                     'RoE_TTM', 'RoE_1y_Fwd', 'RoE_2y_Fwd',\n",
    "                                     'CoE', 'Assumed_G',\n",
    "                                     'Sales_EV_1y_Fwd','EBITDA_EV_1y_Fwd',\n",
    "                                     'EY_TTM','EY_1y_Fwd','EY_2y_Fwd',\n",
    "                                     'BY_TTM','BY_1y_Fwd','BY_2y_Fwd',\n",
    "                                     'DY_TTM','DY_1y_Fwd','DY_2y_Fwd','NOSH_Chg_LTM']].median().reset_index()\n",
    "\n",
    "MedianIndex_df.columns = [f'{col}_Median' if col in ['TR',\n",
    "                                          'NTM_RevGrowth','2y_RevGrowth','NTM_RevGrowth_3mChg_SOM','NTM_Rev_3mChg_SOM',\n",
    "                                          'NTM_EBITDA_Margin','2y_EBITDA_Margin','NTM_EBITDA_Margin_3mChg_SOM',\n",
    "                                          'RoE_TTM', 'RoE_1y_Fwd', 'RoE_2y_Fwd',\n",
    "                                          'CoE', 'Assumed_G',\n",
    "                                          'Sales_EV_1y_Fwd','EBITDA_EV_1y_Fwd',\n",
    "                                          'EY_TTM','EY_1y_Fwd','EY_2y_Fwd',\n",
    "                                          'BY_TTM','BY_1y_Fwd','BY_2y_Fwd',\n",
    "                                          'DY_TTM','DY_1y_Fwd','DY_2y_Fwd','NOSH_Chg_LTM'] else col for col in MedianIndex_df.columns]\n",
    "\n",
    "MedianIndex_df['EV_Sales_1y_Fwd_Median'] = 1/MedianIndex_df['Sales_EV_1y_Fwd_Median']\n",
    "MedianIndex_df['EV_EBITDA_1y_Fwd_Median'] = 1/MedianIndex_df['EBITDA_EV_1y_Fwd_Median']\n",
    "MedianIndex_df['PE_TTM_Median'] = 1/MedianIndex_df['EY_TTM_Median']\n",
    "MedianIndex_df['PE_1y_Fwd_Median'] = 1/MedianIndex_df['EY_1y_Fwd_Median']\n",
    "MedianIndex_df['PE_2y_Fwd_Median'] = 1/MedianIndex_df['EY_2y_Fwd_Median']\n",
    "MedianIndex_df['PB_TTM_Median'] = 1/MedianIndex_df['BY_TTM_Median']\n",
    "MedianIndex_df['PB_1y_Fwd_Median'] = 1/MedianIndex_df['BY_1y_Fwd_Median']\n",
    "MedianIndex_df['PB_2y_Fwd_Median'] = 1/MedianIndex_df['BY_2y_Fwd_Median']\n",
    "\n",
    "### MKT CAP WEIGHTED CALCULATION\n",
    "\n",
    "# Step 1: Create a function for weighted average calculation\n",
    "def weighted_average(df, value_col, weight_col):\n",
    "    valid_entries = ~df[value_col].isna()\n",
    "    weights = df.loc[valid_entries, weight_col]\n",
    "    values = df.loc[valid_entries, value_col]\n",
    "    if weights.sum() == 0:\n",
    "        # Handle the case where denominator is zero\n",
    "        return 0\n",
    "    else:\n",
    "        weighted_avg = (weights * values).sum() / weights.sum()\n",
    "        return weighted_avg\n",
    "\n",
    "# Step 2: Create a function to calculate the market cap weighted averages\n",
    "def calculate_weighted_averages(group):\n",
    "    weighted_TR = weighted_average(group, 'TR', 'Mkt_Cap_SOM')\n",
    "    weighted_NTM_RevGrowth = weighted_average(group, 'NTM_RevGrowth', 'Mkt_Cap')\n",
    "    weighted_2y_RevGrowth = weighted_average(group, '2y_RevGrowth', 'Mkt_Cap')\n",
    "    weighted_NTM_EBITDA_Margin = weighted_average(group, 'NTM_EBITDA_Margin', 'Mkt_Cap')\n",
    "    weighted_2y_EBITDA_Margin = weighted_average(group, '2y_EBITDA_Margin', 'Mkt_Cap')\n",
    "    weighted_RoE_TTM = weighted_average(group, 'RoE_TTM', 'Mkt_Cap')\n",
    "    weighted_RoE_1y_Fwd = weighted_average(group, 'RoE_1y_Fwd', 'Mkt_Cap')\n",
    "    weighted_RoE_2y_Fwd = weighted_average(group, 'RoE_2y_Fwd', 'Mkt_Cap')\n",
    "    weighted_CoE = weighted_average(group, 'CoE', 'Mkt_Cap')\n",
    "    weighted_Assumed_G = weighted_average(group, 'Assumed_G', 'Mkt_Cap')\n",
    "    weighted_Sales_EV_1y_Fwd = weighted_average(group, 'Sales_EV_1y_Fwd', 'Mkt_Cap')\n",
    "    weighted_EBITDA_EV_1y_Fwd = weighted_average(group, 'EBITDA_EV_1y_Fwd', 'Mkt_Cap')\n",
    "    weighted_EY_TTM = weighted_average(group, 'EY_TTM', 'Mkt_Cap')\n",
    "    weighted_EY_1y_Fwd = weighted_average(group, 'EY_1y_Fwd', 'Mkt_Cap')\n",
    "    weighted_EY_2y_Fwd = weighted_average(group, 'EY_2y_Fwd', 'Mkt_Cap') \n",
    "    weighted_BY_TTM = weighted_average(group, 'BY_TTM', 'Mkt_Cap')\n",
    "    weighted_BY_1y_Fwd = weighted_average(group, 'BY_1y_Fwd', 'Mkt_Cap')\n",
    "    weighted_BY_2y_Fwd = weighted_average(group, 'BY_2y_Fwd', 'Mkt_Cap') \n",
    "    weighted_DY_TTM = weighted_average(group, 'DY_TTM', 'Mkt_Cap')\n",
    "    weighted_DY_1y_Fwd = weighted_average(group, 'DY_1y_Fwd', 'Mkt_Cap')\n",
    "    weighted_DY_2y_Fwd = weighted_average(group, 'DY_2y_Fwd', 'Mkt_Cap')\n",
    "    weighted_NOSH_Chg_LTM = weighted_average(group, 'NOSH_Chg_LTM', 'Mkt_Cap')\n",
    "    \n",
    "    \n",
    "    return pd.Series({\n",
    "        'TR_MV_Wgt': weighted_TR,\n",
    "        'NTM_RevGrowth_MV_Wgt': weighted_NTM_RevGrowth,\n",
    "        '2y_RevGrowth_MV_Wgt': weighted_2y_RevGrowth,\n",
    "        'NTM_EBITDA_Margin_MV_Wgt': weighted_NTM_EBITDA_Margin,\n",
    "        '2y_EBITDA_Margin_MV_Wgt': weighted_2y_EBITDA_Margin,\n",
    "        'RoE_TTM_MV_Wgt': weighted_RoE_TTM,\n",
    "        'RoE_1y_Fwd_MV_Wgt': weighted_RoE_1y_Fwd,\n",
    "        'RoE_2y_Fwd_MV_Wgt': weighted_RoE_2y_Fwd,\n",
    "        'CoE_MV_Wgt': weighted_CoE,\n",
    "        'Assumed_G_MV_Wgt': weighted_Assumed_G,\n",
    "        'EV_Sales_1y_Fwd_MV_Wgt': weighted_Sales_EV_1y_Fwd, # execution of reciprocal below\n",
    "        'EV_EBITDA_1y_Fwd_MV_Wgt': weighted_EBITDA_EV_1y_Fwd, # execution of reciprocal below\n",
    "        'PE_TTM_MV_Wgt': weighted_EY_TTM, # execution of reciprocal below\n",
    "        'PE_1y_Fwd_MV_Wgt': weighted_EY_1y_Fwd, # execution of reciprocal below\n",
    "        'PE_2y_Fwd_MV_Wgt': weighted_EY_2y_Fwd, # execution of reciprocal below\n",
    "        'PB_TTM_MV_Wgt': weighted_BY_TTM, # execution of reciprocal below\n",
    "        'PB_1y_Fwd_MV_Wgt': weighted_BY_1y_Fwd, # execution of reciprocal below\n",
    "        'PB_2y_Fwd_MV_Wgt': weighted_BY_2y_Fwd, # execution of reciprocal below\n",
    "        'DY_TTM_MV_Wgt': weighted_DY_TTM,\n",
    "        'DY_1y_Fwd_MV_Wgt': weighted_DY_1y_Fwd,\n",
    "        'DY_2y_Fwd_MV_Wgt': weighted_DY_2y_Fwd,\n",
    "        'NOSH_Chg_LTM_MV_Wgt': weighted_NOSH_Chg_LTM\n",
    "    })\n",
    "\n",
    "# Step 3: Apply the calculation to your DataFrame\n",
    "Index_df = df.groupby('Date').apply(calculate_weighted_averages).reset_index()\n",
    "Index_df['EV_Sales_1y_Fwd_MV_Wgt'] = 1/Index_df['EV_Sales_1y_Fwd_MV_Wgt']\n",
    "Index_df['EV_EBITDA_1y_Fwd_MV_Wgt'] = 1/Index_df['EV_EBITDA_1y_Fwd_MV_Wgt']\n",
    "Index_df['PE_TTM_MV_Wgt'] = 1/Index_df['PE_TTM_MV_Wgt']\n",
    "Index_df['PE_1y_Fwd_MV_Wgt'] = 1/Index_df['PE_1y_Fwd_MV_Wgt']\n",
    "Index_df['PE_2y_Fwd_MV_Wgt'] = 1/Index_df['PE_2y_Fwd_MV_Wgt']\n",
    "Index_df['PB_TTM_MV_Wgt'] = 1/Index_df['PB_1y_Fwd_MV_Wgt']\n",
    "Index_df['PB_1y_Fwd_MV_Wgt'] = 1/Index_df['PB_1y_Fwd_MV_Wgt']\n",
    "Index_df['PB_2y_Fwd_MV_Wgt'] = 1/Index_df['PB_2y_Fwd_MV_Wgt']\n",
    "\n",
    "# Step 4: Add in the equal weighted calculations from above into the Index dataframe\n",
    "Index_df['TR_Eq_Wgt'] = EqualWeightIndex_df['TR_Eq_Wgt']\n",
    "Index_df['NTM_RevGrowth_Eq_Wgt'] = EqualWeightIndex_df['NTM_RevGrowth_Eq_Wgt']\n",
    "Index_df['2y_RevGrowth_Eq_Wgt'] = EqualWeightIndex_df['2y_RevGrowth_Eq_Wgt']\n",
    "Index_df['NTM_EBITDA_Margin_Eq_Wgt'] = EqualWeightIndex_df['NTM_EBITDA_Margin_Eq_Wgt']\n",
    "Index_df['2y_EBITDA_Margin_Eq_Wgt'] = EqualWeightIndex_df['2y_EBITDA_Margin_Eq_Wgt']\n",
    "Index_df['RoE_TTM_Eq_Wgt'] = EqualWeightIndex_df['RoE_TTM_Eq_Wgt']\n",
    "Index_df['RoE_1y_Fwd_Eq_Wgt'] = EqualWeightIndex_df['RoE_1y_Fwd_Eq_Wgt']\n",
    "Index_df['RoE_2y_Fwd_Eq_Wgt'] = EqualWeightIndex_df['RoE_2y_Fwd_Eq_Wgt']\n",
    "Index_df['CoE_Eq_Wgt'] = EqualWeightIndex_df['CoE_Eq_Wgt']\n",
    "Index_df['Assumed_G_Eq_Wgt'] = EqualWeightIndex_df['Assumed_G_Eq_Wgt']\n",
    "Index_df['EV_Sales_1y_Fwd_Eq_Wgt'] = EqualWeightIndex_df['EV_Sales_1y_Fwd_Eq_Wgt']\n",
    "Index_df['EV_EBITDA_1y_Fwd_Eq_Wgt'] = EqualWeightIndex_df['EV_EBITDA_1y_Fwd_Eq_Wgt']\n",
    "Index_df['PE_TTM_Eq_Wgt'] = EqualWeightIndex_df['PE_TTM_Eq_Wgt']\n",
    "Index_df['PE_1y_Fwd_Eq_Wgt'] = EqualWeightIndex_df['PE_1y_Fwd_Eq_Wgt']\n",
    "Index_df['PE_2y_Fwd_Eq_Wgt'] = EqualWeightIndex_df['PE_2y_Fwd_Eq_Wgt']\n",
    "Index_df['PB_TTM_Eq_Wgt'] = EqualWeightIndex_df['PB_TTM_Eq_Wgt']\n",
    "Index_df['PB_1y_Fwd_Eq_Wgt'] = EqualWeightIndex_df['PB_1y_Fwd_Eq_Wgt']\n",
    "Index_df['PB_2y_Fwd_Eq_Wgt'] = EqualWeightIndex_df['PB_2y_Fwd_Eq_Wgt']\n",
    "Index_df['DY_TTM_Eq_Wgt'] = EqualWeightIndex_df['DY_TTM_Eq_Wgt']\n",
    "Index_df['DY_1y_Fwd_Eq_Wgt'] = EqualWeightIndex_df['DY_1y_Fwd_Eq_Wgt']\n",
    "Index_df['DY_2y_Fwd_Eq_Wgt'] = EqualWeightIndex_df['DY_2y_Fwd_Eq_Wgt']\n",
    "Index_df['NOSH_Chg_LTM_Eq_Wgt'] = EqualWeightIndex_df['NOSH_Chg_LTM_Eq_Wgt']\n",
    "\n",
    "# Step 5: Add in the median calculations from above into the Index dataframe\n",
    "Index_df['TR_Median'] = MedianIndex_df['TR_Median']\n",
    "Index_df['NTM_RevGrowth_Median'] = MedianIndex_df['NTM_RevGrowth_Median']\n",
    "Index_df['2y_RevGrowth_Median'] = MedianIndex_df['2y_RevGrowth_Median']\n",
    "Index_df['NTM_EBITDA_Margin_Median'] = MedianIndex_df['NTM_EBITDA_Margin_Median']\n",
    "Index_df['2y_EBITDA_Margin_Median'] = MedianIndex_df['2y_EBITDA_Margin_Median']\n",
    "Index_df['RoE_TTM_Median'] = MedianIndex_df['RoE_TTM_Median']\n",
    "Index_df['RoE_1y_Fwd_Median'] = MedianIndex_df['RoE_1y_Fwd_Median']\n",
    "Index_df['RoE_2y_Fwd_Median'] = MedianIndex_df['RoE_2y_Fwd_Median']\n",
    "Index_df['CoE_Median'] = MedianIndex_df['CoE_Median']\n",
    "Index_df['Assumed_G_Median'] = MedianIndex_df['Assumed_G_Median']\n",
    "Index_df['EV_Sales_1y_Fwd_Median'] = MedianIndex_df['EV_Sales_1y_Fwd_Median']\n",
    "Index_df['EV_EBITDA_1y_Fwd_Median'] = MedianIndex_df['EV_EBITDA_1y_Fwd_Median']\n",
    "Index_df['PE_TTM_Median'] = MedianIndex_df['PE_TTM_Median']\n",
    "Index_df['PE_1y_Fwd_Median'] = MedianIndex_df['PE_1y_Fwd_Median']\n",
    "Index_df['PE_2y_Fwd_Median'] = MedianIndex_df['PE_2y_Fwd_Median']\n",
    "Index_df['PB_TTM_Median'] = MedianIndex_df['PB_TTM_Median']\n",
    "Index_df['PB_1y_Fwd_Median'] = MedianIndex_df['PB_1y_Fwd_Median']\n",
    "Index_df['PB_2y_Fwd_Median'] = MedianIndex_df['PB_2y_Fwd_Median']\n",
    "Index_df['DY_TTM_Median'] = MedianIndex_df['DY_TTM_Median']\n",
    "Index_df['DY_1y_Fwd_Median'] = MedianIndex_df['DY_1y_Fwd_Median']\n",
    "Index_df['DY_2y_Fwd_Median'] = MedianIndex_df['DY_2y_Fwd_Median']\n",
    "Index_df['NOSH_Chg_LTM_Median'] = MedianIndex_df['NOSH_Chg_LTM_Median']\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "#Index_df.tail()\n",
    "\n",
    "# Export to xlxs\n",
    "full_export_path = export_path + export_index_file_name \n",
    "Index_df.to_excel(full_export_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94420f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LONG / SHORT RETURNS BY VARIABLE ###\n",
    "\n",
    "# List of target columns you want to iterate over\n",
    "target_columns = ['Mkt_Cap_SOM','TR_LTM_SOM','TR_L6M_SOM','TR_L3M_SOM',\n",
    "                  'NTM_RevGrowth_SOM','NTM_EBITDA_Margin_SOM','NTM_RoE_SOM','NTM_RevGrowth_3mChg_SOM','NTM_Rev_3mChg_SOM',\n",
    "                  'NTM_EBITDA_Margin_3mChg_SOM','Sales_EV_1y_Fwd_SOM',\n",
    "                  'EBITDA_EV_1y_Fwd_SOM','EY_1y_Fwd_SOM','EY_2y_Fwd_SOM','BY_1y_Fwd_SOM','CoE_SOM']            \n",
    "\n",
    "# Initialize the final result DataFrame with unique Dates\n",
    "LS_results_df = pd.DataFrame(df['Date'].unique(), columns=['Date'])\n",
    "\n",
    "# Loop over each target column\n",
    "for target_column in target_columns:\n",
    "    \n",
    "    # Ensure both the target column and 'TR' are numeric\n",
    "    df[target_column] = pd.to_numeric(df[target_column], errors='coerce')\n",
    "    df['TR'] = pd.to_numeric(df['TR'], errors='coerce')\n",
    "    \n",
    "    # List to collect results for this target column\n",
    "    results = []\n",
    "    \n",
    "    # Group by 'Date' and compute median and means for 'Long_' and 'Short_' prefixed series\n",
    "    for date, group in df.groupby('Date'):\n",
    "        median_value = group[target_column].median()\n",
    "        \n",
    "        # Long: mean TR where target_column is above the median\n",
    "        long_mean_tr = group[group[target_column] > median_value]['TR'].mean()\n",
    "        \n",
    "        # Short: mean TR where target_column is below the median\n",
    "        short_mean_tr = group[group[target_column] < median_value]['TR'].mean()\n",
    "        \n",
    "        # Append the results for each date\n",
    "        results.append({\n",
    "            'Date': date,\n",
    "            f'Long_{target_column}': long_mean_tr,  # Dynamically naming the column\n",
    "            f'Short_{target_column}': short_mean_tr  # Dynamically naming the column\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame with the results for the current target column\n",
    "    target_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add the \"LS_<target_column>\" column\n",
    "    target_df[f'LS_{target_column}'] = (1 + target_df[f'Long_{target_column}']) / (1 + target_df[f'Short_{target_column}']) - 1\n",
    "    \n",
    "    # Merge the current results with the final results DataFrame\n",
    "    LS_results_df = pd.merge(LS_results_df, target_df, on='Date', how='left')\n",
    "\n",
    "# Output the final result\n",
    "#print(LS_results_df)\n",
    "\n",
    "# Export to xlxs\n",
    "full_export_path = export_path + export_LS_file_name \n",
    "LS_results_df.to_excel(full_export_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc6ff0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### VALUATION DISPERSION BY VARIABLE ###\n",
    "\n",
    "\n",
    "# List of target columns you want to iterate over\n",
    "target_columns = ['NTM_RevGrowth', '2y_RevGrowth',\n",
    "                  'NTM_EBITDA_Margin','2y_EBITDA_Margin',\n",
    "                  'RoE_1y_Fwd','RoE_2y_Fwd',\n",
    "                  'CoE','Assumed_G',\n",
    "                  'Sales_EV_1y_Fwd','EBITDA_EV_1y_Fwd','EY_1y_Fwd','EY_2y_Fwd',]\n",
    "\n",
    "# Initialize the final result DataFrame with unique Dates\n",
    "dispersion_results_df = pd.DataFrame(df['Date'].unique(), columns=['Date'])\n",
    "\n",
    "# Loop over each target column\n",
    "for target_column in target_columns:\n",
    "    \n",
    "    # Ensure both the target column and 'TR' are numeric\n",
    "    df[target_column] = pd.to_numeric(df[target_column], errors='coerce')\n",
    "    \n",
    "    # List to collect results for this target column\n",
    "    results = []\n",
    "    \n",
    "    # Group by 'Date' and compute median and means for 'Long_' and 'Short_' prefixed series\n",
    "    for date, group in df.groupby('Date'):\n",
    "        median_value = group[target_column].median()\n",
    "        \n",
    "        # Long: mean of target_column where target_column is above the median\n",
    "        long_mean_target = group[group[target_column] > median_value][target_column].mean()\n",
    "        \n",
    "        # Short: mean of target_column where target_column is below the median\n",
    "        short_mean_target = group[group[target_column] < median_value][target_column].mean()\n",
    "        \n",
    "        # Append the results for each date\n",
    "        results.append({\n",
    "            'Date': date,\n",
    "            f'above_{target_column}': long_mean_target,  # Dynamically naming the column\n",
    "            f'below_{target_column}': short_mean_target  # Dynamically naming the column\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame with the results for the current target column\n",
    "    target_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add the \"LS_<target_column>\" column\n",
    "    target_df[f'spread_{target_column}'] = (1 + target_df[f'above_{target_column}']) / (1 + target_df[f'below_{target_column}']) - 1\n",
    "    \n",
    "    # Merge the current results with the final results DataFrame\n",
    "    dispersion_results_df = pd.merge(dispersion_results_df, target_df, on='Date', how='left')\n",
    "\n",
    "# Output the final result\n",
    "#print(dispersion_results_df)\n",
    "\n",
    "# Export to xlxs\n",
    "full_export_path = export_path + export_dispersion_file_name \n",
    "dispersion_results_df.to_excel(full_export_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10ef20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine stock by stock data for a single month\n",
    "examine_data = df[(df['Date'] >= '2024-09-30') & (df['Date'] <= '2024-09-30')]\n",
    "\n",
    "# Export to xlsx\n",
    "full_export_path = export_path + export_examine_CSV_file_name\n",
    "examine_data.to_excel(full_export_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
