{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f66adc41-f595-45de-83f5-11443bba525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FTSE250] Excel export written to: H:/Tech Hardware Shared/$Mike/Quant/Python_Outputs/QC\\FTSE250_Filtered_Net_NAV_and_constituents.xlsx\n",
      "[SnP500] Excel export written to: H:/Tech Hardware Shared/$Mike/Quant/Python_Outputs/QC\\SnP500_Filtered_Net_NAV_and_constituents.xlsx\n",
      "[STOXX600] Excel export written to: H:/Tech Hardware Shared/$Mike/Quant/Python_Outputs/QC\\STOXX600_Filtered_Net_NAV_and_constituents.xlsx\n",
      "[S&PMidCap] Excel export written to: H:/Tech Hardware Shared/$Mike/Quant/Python_Outputs/QC\\S&PMidCap_Filtered_Net_NAV_and_constituents.xlsx\n"
     ]
    }
   ],
   "source": [
    "# QC Backtest + Live Portfolio (latest-month) – full script\n",
    "# Version: v6 (2025-08-26)\n",
    "# Changes in v6:\n",
    "#  - Add FACTOR_TOP knob (half/third/quarter/all or numeric fraction) + FACTOR_MIN_K\n",
    "#  - Backtest & LIVE meta-selection use compute_top_k(...)\n",
    "#  - Keeps v5 robust date parser + earlier numeric-coercion fixes and Weight_% assign\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "# ---------- Robust date parser to avoid pandas 'Could not infer format' warnings ----------\n",
    "def parse_date_column(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Parse a date column that may contain Excel serials and/or multiple string formats\n",
    "    without triggering the pandas 'Could not infer format' warning.\n",
    "    Tries in order: Excel serial, then a list of explicit string formats, then a last-resort\n",
    "    dateutil parse with dayfirst=True. Always returns a datetime64[ns] Series (NaT on failure).\n",
    "    \"\"\"\n",
    "    s = series.copy()\n",
    "    # 1) Try Excel serials (vectorized)\n",
    "    as_num = pd.to_numeric(s, errors='coerce')\n",
    "    out = pd.to_datetime(as_num, unit='D', origin='1899-12-30', errors='coerce')\n",
    "\n",
    "    # 2) Try explicit string formats (vectorized per-format)\n",
    "    formats = [\n",
    "        '%Y-%m-%d',    # 2025-08-31\n",
    "        '%d-%m-%Y',    # 31-08-2025\n",
    "        '%d/%m/%Y',    # 31/08/2025\n",
    "        '%m/%d/%Y',    # 08/31/2025\n",
    "        '%d-%b-%Y',    # 31-Aug-2025\n",
    "        '%d-%b-%y',    # 31-Aug-25\n",
    "    ]\n",
    "    mask = out.isna()\n",
    "    s_str = s.astype(str)\n",
    "    for fmt in formats:\n",
    "        if not mask.any():\n",
    "            break\n",
    "        idx = mask\n",
    "        parsed = pd.to_datetime(s_str[idx], format=fmt, errors='coerce')\n",
    "        out.loc[idx] = out.loc[idx].fillna(parsed)\n",
    "        mask = out.isna()\n",
    "\n",
    "    # 3) Last resort: dateutil with dayfirst, still vectorized; may be slower but no warning now\n",
    "    if mask.any():\n",
    "        out.loc[mask] = pd.to_datetime(s_str[mask], errors='coerce', dayfirst=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "# ======================================\n",
    "# 1) Which universes to run (edit later)\n",
    "# ======================================\n",
    "UNIVERSES = [\"FTSE250\", \"SnP500\", \"STOXX600\",\"S&PMidCap\"]          \n",
    "\n",
    "# ======================================\n",
    "# 2) Portfolio knobs (edit anytime)\n",
    "# ======================================\n",
    "KEEP_N       = 10        # number of stocks per factor portfolio\n",
    "SCREEN_N     = 20        # number of candidates screened per month\n",
    "FACTOR_TOP   = 'quarter'\n",
    "FACTOR_MIN_K = 1         # never pick fewer than this many factors\n",
    "\n",
    "# ---- helpers for meta-selection ----\n",
    "def _factor_fraction(x) -> float:\n",
    "    \"\"\"Resolve FACTOR_TOP to a numeric fraction in (0, 1].\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        mapping = {\n",
    "            'half': 0.5,\n",
    "            'third': 1/3,\n",
    "            'quarter': 0.25,\n",
    "            'fifth': 0.20,\n",
    "            'all': 1.0,\n",
    "        }\n",
    "        if s in mapping:\n",
    "            return mapping[s]\n",
    "        try:\n",
    "            f = float(s)\n",
    "        except ValueError:\n",
    "            f = 0.5\n",
    "        return max(1e-9, min(1.0, f))\n",
    "    try:\n",
    "        f = float(x)\n",
    "    except Exception:\n",
    "        f = 0.5\n",
    "    return max(1e-9, min(1.0, f))\n",
    "\n",
    "def compute_top_k(n_available: int) -> int:\n",
    "    \"\"\"Number of factors to keep given how many have valid 12m returns.\"\"\"\n",
    "    frac = _factor_fraction(FACTOR_TOP)\n",
    "    return max(FACTOR_MIN_K, int(np.ceil(n_available * frac)))\n",
    "\n",
    "# ======================================\n",
    "# 3) Shared base folders (edit if needed)\n",
    "# ======================================\n",
    "BASE        = \"H:/Tech Hardware Shared/$Mike/Quant\"\n",
    "CONSTIT_DIR = f\"{BASE}/Constituents\"\n",
    "CSV_DIR     = f\"{BASE}/CSV_files/QC\"       # per-ISIN history files (unchanged)\n",
    "SPOT_DIR    = f\"{BASE}/Spot\"\n",
    "EXPORT_DIR  = f\"{BASE}/Python_Outputs/QC\"\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "# ======================================\n",
    "# 4) Trading costs by universe\n",
    "# ======================================\n",
    "COSTS = {\n",
    "    \"FTSE250\":     {\"STAMP_DUTY\": 0.0050, \"BID_ASK\": 0.0010, \"FEES\": 0.0010},\n",
    "    \"STOXX600\":    {\"STAMP_DUTY\": 0.0000, \"BID_ASK\": 0.0010, \"FEES\": 0.0010},\n",
    "    \"SnP500\":      {\"STAMP_DUTY\": 0.0000, \"BID_ASK\": 0.0010, \"FEES\": 0.0010},\n",
    "    \"S&PMidCap\":   {\"STAMP_DUTY\": 0.0000, \"BID_ASK\": 0.0010, \"FEES\": 0.0010},    \n",
    "}\n",
    "\n",
    "# ======================================\n",
    "# 5) Main runner for one universe (full pipeline)\n",
    "# ======================================\n",
    "def run_universe(UNIVERSE: str, keep_n: int = KEEP_N, screen_n: int = SCREEN_N):\n",
    "    if UNIVERSE not in COSTS:\n",
    "        print(f\"[SKIP] Unknown UNIVERSE '{UNIVERSE}'. Supported: {list(COSTS)}\")\n",
    "        return\n",
    "\n",
    "    # --- config & file paths ---\n",
    "    constituents_file_path = os.path.join(CONSTIT_DIR, f\"{UNIVERSE}_Constit.csv\")\n",
    "    spot_file_path         = os.path.join(SPOT_DIR,     f\"{UNIVERSE}_Spot.csv\")\n",
    "    export_xls             = os.path.join(EXPORT_DIR,   f\"{UNIVERSE}_Filtered_Net_NAV_and_constituents.xlsx\")\n",
    "\n",
    "    # --- trading costs ---\n",
    "    STAMP_DUTY = COSTS[UNIVERSE][\"STAMP_DUTY\"]\n",
    "    BID_ASK    = COSTS[UNIVERSE][\"BID_ASK\"]\n",
    "    FEES       = COSTS[UNIVERSE][\"FEES\"]\n",
    "    TRADING_COST_RATE = STAMP_DUTY + BID_ASK + FEES\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) Build constituent schedule\n",
    "    # -----------------------------\n",
    "    if not os.path.exists(constituents_file_path):\n",
    "        print(f\"[{UNIVERSE}] Constituents file not found: {constituents_file_path}  → skipping.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(constituents_file_path, header=0)\n",
    "    df.columns = pd.to_datetime(df.columns, format='%d-%b-%y')\n",
    "    df = pd.melt(df, id_vars=[], var_name='Date', value_name='ISIN')\n",
    "    df['Date'] = df['Date'] + pd.offsets.MonthEnd(0)\n",
    "    df['ISIN'] = df['ISIN'].fillna('placeholder')\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) Read Spot (intramonth) rows\n",
    "    # -----------------------------\n",
    "    if os.path.exists(spot_file_path):\n",
    "        spot_df = pd.read_csv(spot_file_path)\n",
    "        spot_df.columns = [c.strip() for c in spot_df.columns]\n",
    "        if 'ISIN' not in spot_df.columns or 'Date' not in spot_df.columns:\n",
    "            raise ValueError(f\"{UNIVERSE}_Spot.csv must include 'ISIN' and 'Date' columns.\")\n",
    "        spot_df['Date'] = parse_date_column(spot_df['Date'])\n",
    "        spot_df = spot_df.dropna(subset=['ISIN', 'Date'])\n",
    "        spot_df['ISIN'] = spot_df['ISIN'].astype(str).str.strip()\n",
    "    else:\n",
    "        spot_df = pd.DataFrame(columns=['ISIN', 'Date'])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Read/augment per-ISIN files\n",
    "    # -----------------------------\n",
    "    all_isin_data = []\n",
    "\n",
    "    for isin in df['ISIN'].unique():\n",
    "        if isin == 'placeholder':\n",
    "            continue\n",
    "\n",
    "        csv_file_path = os.path.join(CSV_DIR, f'{isin}.csv')\n",
    "        if not os.path.exists(csv_file_path):\n",
    "            continue\n",
    "\n",
    "        hist_df = pd.read_csv(csv_file_path, header=0, na_values=[\"-\"])\n",
    "        # Convert historical Excel serial date to datetime (coerces ISO safely)\n",
    "        hist_df['Date'] = pd.to_datetime(hist_df['Date'], unit='D', origin='1899-12-30', errors='coerce')\n",
    "\n",
    "        # Append matching spot rows (if any)\n",
    "        if not spot_df.empty:\n",
    "            spot_rows = spot_df[spot_df['ISIN'] == isin].copy()\n",
    "            if not spot_rows.empty:\n",
    "                if 'ISIN' not in hist_df.columns:\n",
    "                    hist_df['ISIN'] = isin\n",
    "                else:\n",
    "                    hist_df['ISIN'] = hist_df['ISIN'].fillna(isin).astype(str).str.strip()\n",
    "\n",
    "                spot_rows['Date'] = pd.to_datetime(spot_rows['Date'], errors='coerce')\n",
    "                spot_rows['ISIN'] = isin\n",
    "\n",
    "                hist_df = pd.concat([hist_df, spot_rows], ignore_index=True, sort=False)\n",
    "                hist_df = (\n",
    "                    hist_df.sort_values('Date')\n",
    "                           .drop_duplicates(subset=['Date'], keep='last')\n",
    "                           .reset_index(drop=True)\n",
    "                )\n",
    "\n",
    "        # Month-end align (latest within month wins)\n",
    "        hist_df['Date'] = hist_df['Date'] + pd.offsets.MonthEnd(0)\n",
    "\n",
    "        # ---- Calculations ----\n",
    "        isin_data = hist_df.copy()\n",
    "        isin_data['ISIN'] = isin\n",
    "\n",
    "        numeric_columns = [\n",
    "            'Mkt_Cap','RI','P',\n",
    "            'Rev_LTM','Rev_NTM',\n",
    "            'EBITDA_LTM_DS','EBITDA_LTM','EBITDA_NTM',\n",
    "            'EPS_LTM','EPS_NTM',\n",
    "            'BPS_LTM','BPS_NTM',\n",
    "            'DPS_LTM','DPS_NTM',\n",
    "            'CF_LTM_DS','CFPS_LTM','CFPS_NTM',\n",
    "            'Assets_LTM',\n",
    "            'NetDebt_LTM_DS','NetDebt_LTM','NetDebt_NTM'\n",
    "        ]\n",
    "        for col in numeric_columns:\n",
    "            if col not in isin_data.columns:\n",
    "                isin_data[col] = np.nan\n",
    "        isin_data[numeric_columns] = isin_data[numeric_columns].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "\n",
    "        isin_data['RI'] = np.where(isin_data['RI'] == 0, np.nan, isin_data['RI'])\n",
    "        isin_data['NetDebt_LTM_Best']  = np.where(pd.notnull(isin_data['NetDebt_LTM']),\n",
    "                                                  isin_data['NetDebt_LTM'],\n",
    "                                                  isin_data['NetDebt_LTM_DS'])\n",
    "        isin_data['EBITDA_LTM_Best']   = np.where(pd.notnull(isin_data['EBITDA_LTM']),\n",
    "                                                  isin_data['EBITDA_LTM'],\n",
    "                                                  isin_data['EBITDA_LTM_DS'])\n",
    "        isin_data['EV'] = isin_data['Mkt_Cap'] + np.where(pd.notnull(isin_data['NetDebt_NTM']),\n",
    "                                                          isin_data['NetDebt_NTM'],\n",
    "                                                          isin_data['NetDebt_LTM_Best'])\n",
    "\n",
    "        for c in ['BPS_LTM','BPS_NTM']:\n",
    "            isin_data[c] = np.where(isin_data[c] < 0, np.nan, isin_data[c])\n",
    "\n",
    "        isin_data['TR']           = isin_data['RI'] / isin_data['RI'].shift(1) - 1\n",
    "        isin_data['TR_LTM']       = isin_data['RI'] / isin_data['RI'].shift(12) - 1\n",
    "        isin_data['EPS_NTM_Chg']  = np.where(isin_data['EPS_NTM'].shift(12) > 0,\n",
    "                                             isin_data['EPS_NTM'] / isin_data['EPS_NTM'].shift(12) - 1,\n",
    "                                             np.nan)\n",
    "\n",
    "        isin_data['NTM_RevGrowth']  = isin_data['Rev_NTM'] / isin_data['Rev_LTM'] - 1\n",
    "        isin_data['Assets_NTM_Raw'] = (1 + isin_data['NTM_RevGrowth']) * isin_data['Assets_LTM']\n",
    "        isin_data['Assets_NTM']     = (isin_data[['Assets_NTM_Raw', 'Assets_LTM']].max(axis=1))\n",
    "\n",
    "        isin_data['RoE_LTM']        = isin_data['EPS_LTM'] / isin_data['BPS_LTM']\n",
    "        isin_data['RoE_LTM_5yAvg']  = isin_data['RoE_LTM'].rolling(window=60, min_periods=60).median()\n",
    "        isin_data['RoE_NTM']        = isin_data['EPS_NTM'] / isin_data['BPS_NTM']\n",
    "        isin_data['RoE_NTM_5yAvg']  = isin_data['RoE_NTM'].rolling(window=60, min_periods=60).median()\n",
    "\n",
    "        isin_data['Sales_EV_LTM']   = isin_data['Rev_LTM'] / isin_data['EV']\n",
    "        isin_data['Sales_EV_NTM']   = isin_data['Rev_NTM'] / isin_data['EV']\n",
    "\n",
    "        isin_data['EBITDA_EV_LTM']  = isin_data['EBITDA_LTM_Best'] / isin_data['EV']\n",
    "        isin_data['EBITDA_EV_NTM']  = isin_data['EBITDA_NTM'] / isin_data['EV']\n",
    "\n",
    "        isin_data['EY_LTM']         = isin_data['EPS_LTM'] / isin_data['P']\n",
    "        isin_data['EY_NTM']         = isin_data['EPS_NTM'] / isin_data['P']\n",
    "\n",
    "        isin_data['BY_LTM']         = isin_data['BPS_LTM'] / isin_data['P']\n",
    "        isin_data['BY_NTM']         = isin_data['BPS_NTM'] / isin_data['P']\n",
    "\n",
    "        isin_data['DY_LTM']         = isin_data['DPS_LTM'] / isin_data['P']\n",
    "        isin_data['DY_NTM']         = isin_data['DPS_NTM'] / isin_data['P']\n",
    "\n",
    "        isin_data['CFY_LTM_IBES']   = isin_data['CFPS_LTM'] / isin_data['P']\n",
    "        isin_data['CFY_LTM_DS']     = isin_data['CF_LTM_DS'] / isin_data['Mkt_Cap']\n",
    "        isin_data['CFY_LTM']        = np.where(pd.notnull(isin_data['CFY_LTM_IBES']),\n",
    "                                               isin_data['CFY_LTM_IBES'],\n",
    "                                               isin_data['CFY_LTM_DS'])\n",
    "        isin_data['CFY_NTM']        = isin_data['CFPS_NTM'] / isin_data['P']\n",
    "        isin_data['CF_LTM']         = isin_data['CFY_LTM'] * isin_data['Mkt_Cap']\n",
    "        isin_data['CF_NTM']         = isin_data['CFY_NTM'] * isin_data['Mkt_Cap']\n",
    "        isin_data['CF_Assets_LTM']  = isin_data['CF_LTM'] / isin_data['Assets_LTM']\n",
    "        isin_data['CF_Assets_NTM']  = isin_data['CF_NTM'] / isin_data['Assets_NTM']\n",
    "        isin_data['CF_Assets_LTM_5yAvg'] = isin_data['CF_Assets_LTM'].rolling(window=60, min_periods=60).median()\n",
    "\n",
    "        isin_data['EBITDA_Assets_LTM']       = isin_data['EBITDA_LTM_Best'] / isin_data['Assets_LTM']\n",
    "        isin_data['EBITDA_Assets_NTM']       = isin_data['EBITDA_NTM'] / isin_data['Assets_NTM']\n",
    "        isin_data['EBITDA_Assets_LTM_5yAvg'] = isin_data['EBITDA_Assets_LTM'].rolling(window=60, min_periods=60).median()\n",
    "\n",
    "        # Clip yields/rates to plausible bands (not multiples)\n",
    "        for c, bounds in {\n",
    "            'Sales_EV_NTM':  {'lower': -0.02, 'upper': 1.0},\n",
    "            'EBITDA_EV_NTM': {'lower': -0.02, 'upper': 1.0},\n",
    "            'EY_LTM':        {'lower': -0.02, 'upper': 1.0},\n",
    "            'EY_NTM':        {'lower': -0.02, 'upper': 1.0},\n",
    "            'RoE_LTM':       {'lower': -0.5,  'upper': 1.5},\n",
    "            'RoE_NTM':       {'lower': -0.5,  'upper': 1.5},\n",
    "        }.items():\n",
    "            isin_data[c] = isin_data[c].clip(**bounds)\n",
    "\n",
    "        # Start-of-month versions\n",
    "        for c in [\n",
    "            'Mkt_Cap', 'EV',\n",
    "            'TR_LTM','EPS_NTM_Chg',\n",
    "            'EBITDA_Assets_LTM','CF_Assets_LTM','RoE_LTM',\n",
    "            'EBITDA_Assets_NTM','CF_Assets_NTM','RoE_NTM',\n",
    "            'EBITDA_Assets_LTM_5yAvg','CF_Assets_LTM_5yAvg','RoE_LTM_5yAvg',\n",
    "            'EBITDA_EV_LTM','Sales_EV_LTM','CFY_LTM','EY_LTM','DY_LTM',\n",
    "            'EBITDA_EV_NTM','Sales_EV_NTM','CFY_NTM','EY_NTM','DY_NTM'\n",
    "        ]:\n",
    "            isin_data[c + '_SOM'] = isin_data[c].shift(1)\n",
    "\n",
    "        all_isin_data.append(isin_data)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) Merge all ISIN data\n",
    "    # -----------------------------\n",
    "    if not all_isin_data:\n",
    "        print(f\"[{UNIVERSE}] No per-ISIN CSVs found in {CSV_DIR}  → skipping.\")\n",
    "        return\n",
    "\n",
    "    isin_final_df = pd.concat(all_isin_data, ignore_index=True)\n",
    "    df = pd.merge(df, isin_final_df, on=['Date','ISIN'])\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) Medians & filters\n",
    "    # -----------------------------\n",
    "    for col in ['EBITDA_Assets_LTM_5yAvg_SOM', 'CF_Assets_LTM_5yAvg_SOM', 'RoE_LTM_5yAvg_SOM']:\n",
    "        df[f\"{col}_Median\"] = df.groupby('Date')[col].transform('median')\n",
    "\n",
    "    df['EBITDA_Assets_LTM_SOM_+'] = np.where(df['EBITDA_Assets_LTM_5yAvg_SOM'] > df['EBITDA_Assets_LTM_5yAvg_SOM_Median'], df['EBITDA_Assets_LTM_SOM'], np.nan)\n",
    "    df['CF_Assets_LTM_SOM_+']     = np.where(df['CF_Assets_LTM_5yAvg_SOM'] > df['CF_Assets_LTM_5yAvg_SOM_Median'],     df['CF_Assets_LTM_SOM'],     np.nan)\n",
    "    df['RoE_LTM_SOM_+']           = np.where(df['RoE_LTM_5yAvg_SOM'] > df['RoE_LTM_5yAvg_SOM_Median'],                 df['RoE_LTM_SOM'],           np.nan)\n",
    "\n",
    "    df['EBITDA_Assets_NTM_SOM_+'] = np.where(df['EBITDA_Assets_LTM_5yAvg_SOM'] > df['EBITDA_Assets_LTM_5yAvg_SOM_Median'], df['EBITDA_Assets_NTM_SOM'], np.nan)\n",
    "    df['CF_Assets_NTM_SOM_+']     = np.where(df['CF_Assets_LTM_5yAvg_SOM'] > df['CF_Assets_LTM_5yAvg_SOM_Median'],     df['CF_Assets_NTM_SOM'],     np.nan)\n",
    "    df['RoE_NTM_SOM_+']           = np.where(df['RoE_LTM_5yAvg_SOM'] > df['RoE_LTM_5yAvg_SOM_Median'],                 df['RoE_NTM_SOM'],           np.nan)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6) Rank columns\n",
    "    # -----------------------------\n",
    "    cols_to_rank = [\n",
    "        'TR_LTM_SOM', 'EPS_NTM_Chg_SOM',\n",
    "        'EBITDA_Assets_LTM_SOM_+','CF_Assets_LTM_SOM_+','RoE_LTM_SOM_+',\n",
    "        'EBITDA_Assets_NTM_SOM_+','CF_Assets_NTM_SOM_+','RoE_NTM_SOM_+',\n",
    "        'Sales_EV_LTM_SOM','EBITDA_EV_LTM_SOM','EY_LTM_SOM','CFY_LTM_SOM','DY_LTM_SOM',\n",
    "        'Sales_EV_NTM_SOM','EBITDA_EV_NTM_SOM','EY_NTM_SOM','CFY_NTM_SOM','DY_NTM_SOM'\n",
    "    ]\n",
    "    for col in cols_to_rank:\n",
    "        df[f\"{col}_Rank\"] = df.groupby('Date')[col].rank(ascending=False, method='min')\n",
    "\n",
    "    # Enforce numeric dtype for freshly created rank columns\n",
    "    rank_cols_made = [f\"{c}_Rank\" for c in cols_to_rank]\n",
    "    df[rank_cols_made] = df[rank_cols_made].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7) Composites (quality / value / CoE / MTUM)\n",
    "    # -----------------------------\n",
    "    quality_plus_LTM_cols = ['EBITDA_Assets_LTM_SOM_+_Rank','CF_Assets_LTM_SOM_+_Rank','RoE_LTM_SOM_+_Rank']\n",
    "    n = len(quality_plus_LTM_cols)\n",
    "    df['quality_+_LTM_Rank'] = df[quality_plus_LTM_cols].apply(lambda r: np.nan if r.count() < (n-1) else r.nsmallest(n-1).mean(), axis=1)\n",
    "\n",
    "    quality_plus_NTM_cols = ['EBITDA_Assets_NTM_SOM_+_Rank','CF_Assets_NTM_SOM_+_Rank','RoE_NTM_SOM_+_Rank']\n",
    "    n = len(quality_plus_NTM_cols)\n",
    "    df['quality_+_NTM_Rank'] = df[quality_plus_NTM_cols].apply(lambda r: np.nan if r.count() < (n-1) else r.nsmallest(n-1).mean(), axis=1)\n",
    "\n",
    "    value_LTM_cols = ['Sales_EV_LTM_SOM_Rank','EBITDA_EV_LTM_SOM_Rank','EY_LTM_SOM_Rank','CFY_LTM_SOM_Rank','DY_LTM_SOM_Rank']\n",
    "    n = len(value_LTM_cols)\n",
    "    df['value_+_LTM_Rank'] = df[value_LTM_cols].apply(lambda r: r.nsmallest(n-2).mean() if r.count() >= (n-2) else np.nan, axis=1)\n",
    "\n",
    "    value_NTM_cols = ['Sales_EV_NTM_SOM_Rank','EBITDA_EV_NTM_SOM_Rank','EY_NTM_SOM_Rank','CFY_NTM_SOM_Rank','DY_NTM_SOM_Rank']\n",
    "    n = len(value_NTM_cols)\n",
    "    df['value_+_NTM_Rank'] = df[value_NTM_cols].apply(lambda r: r.nsmallest(n-2).mean() if r.count() >= (n-2) else np.nan, axis=1)\n",
    "\n",
    "    df['CoE_+_LTM_score'] = df['quality_+_LTM_Rank'] + df['value_+_LTM_Rank']\n",
    "    df['CoE_+_LTM_Rank']  = df.groupby('Date')['CoE_+_LTM_score'].rank(ascending=True, method='min')\n",
    "    df['CoE_+_NTM_score'] = df['quality_+_NTM_Rank'] + df['value_+_NTM_Rank']\n",
    "    df['CoE_+_NTM_Rank']  = df.groupby('Date')['CoE_+_NTM_score'].rank(ascending=True, method='min')\n",
    "\n",
    "    df['quality_+_LTM_TR_MTUM_score'] = df['quality_+_LTM_Rank'] + df['TR_LTM_SOM_Rank']\n",
    "    df['quality_+_LTM_TR_MTUM']       = df.groupby('Date')['quality_+_LTM_TR_MTUM_score'].rank(ascending=True, method='min')\n",
    "    df['quality_+_NTM_TR_MTUM_score'] = df['quality_+_NTM_Rank'] + df['TR_LTM_SOM_Rank']\n",
    "    df['quality_+_NTM_TR_MTUM']       = df.groupby('Date')['quality_+_NTM_TR_MTUM_score'].rank(ascending=True, method='min')\n",
    "\n",
    "    df['quality_+_LTM_EPS_MTUM_score'] = df['quality_+_LTM_Rank'] + df['EPS_NTM_Chg_SOM_Rank']\n",
    "    df['quality_+_LTM_EPS_MTUM']       = df.groupby('Date')['quality_+_LTM_EPS_MTUM_score'].rank(ascending=True, method='min')\n",
    "    df['quality_+_NTM_EPS_MTUM_score'] = df['quality_+_NTM_Rank'] + df['EPS_NTM_Chg_SOM_Rank']\n",
    "    df['quality_+_NTM_EPS_MTUM']       = df.groupby('Date')['quality_+_NTM_EPS_MTUM_score'].rank(ascending=True, method='min')\n",
    "\n",
    "    df['value_+_LTM_TR_MTUM_score'] = df['value_+_LTM_Rank'] + df['TR_LTM_SOM_Rank']\n",
    "    df['value_+_LTM_TR_MTUM']       = df.groupby('Date')['value_+_LTM_TR_MTUM_score'].rank(ascending=True, method='min')\n",
    "    df['value_+_NTM_TR_MTUM_score'] = df['value_+_NTM_Rank'] + df['TR_LTM_SOM_Rank']\n",
    "    df['value_+_NTM_TR_MTUM']       = df.groupby('Date')['value_+_NTM_TR_MTUM_score'].rank(ascending=True, method='min')\n",
    "\n",
    "    df['value_+_LTM_EPS_MTUM_score'] = df['value_+_LTM_Rank'] + df['EPS_NTM_Chg_SOM_Rank']\n",
    "    df['value_+_LTM_EPS_MTUM']       = df.groupby('Date')['value_+_LTM_EPS_MTUM_score'].rank(ascending=True, method='min')\n",
    "    df['value_+_NTM_EPS_MTUM_score'] = df['value_+_NTM_Rank'] + df['EPS_NTM_Chg_SOM_Rank']\n",
    "    df['value_+_NTM_EPS_MTUM']       = df.groupby('Date')['value_+_NTM_EPS_MTUM_score'].rank(ascending=True, method='min')\n",
    "\n",
    "    df['CoE_+_LTM_TR_MTUM_score'] = df['CoE_+_LTM_Rank'] + df['TR_LTM_SOM_Rank']\n",
    "    df['CoE_+_LTM_TR_MTUM']       = df.groupby('Date')['CoE_+_LTM_TR_MTUM_score'].rank(ascending=True, method='min')\n",
    "    df['CoE_+_NTM_TR_MTUM_score'] = df['CoE_+_NTM_Rank'] + df['TR_LTM_SOM_Rank']\n",
    "    df['CoE_+_NTM_TR_MTUM']       = df.groupby('Date')['CoE_+_NTM_TR_MTUM_score'].rank(ascending=True, method='min')\n",
    "\n",
    "    df['CoE_+_LTM_EPS_MTUM_score'] = df['CoE_+_LTM_Rank'] + df['EPS_NTM_Chg_SOM_Rank']\n",
    "    df['CoE_+_LTM_EPS_MTUM']       = df.groupby('Date')['CoE_+_LTM_EPS_MTUM_score'].rank(ascending=True, method='min')\n",
    "    df['CoE_+_NTM_EPS_MTUM_score'] = df['CoE_+_NTM_Rank'] + df['EPS_NTM_Chg_SOM_Rank']\n",
    "    df['CoE_+_NTM_EPS_MTUM']       = df.groupby('Date')['CoE_+_NTM_EPS_MTUM_score'].rank(ascending=True, method='min')\n",
    "\n",
    "    df['CoE_TR_LTM_score'] = df['quality_+_LTM_TR_MTUM'] + df['value_+_LTM_TR_MTUM']\n",
    "    df['CoE_TR_LTM_Rank']  = df.groupby('Date')['CoE_TR_LTM_score'].rank(ascending=True, method='min')\n",
    "    df['CoE_TR_NTM_score'] = df['quality_+_NTM_TR_MTUM'] + df['value_+_NTM_TR_MTUM']\n",
    "    df['CoE_TR_NTM_Rank']  = df.groupby('Date')['CoE_TR_NTM_score'].rank(ascending=True, method='min')\n",
    "\n",
    "    df['CoE_EPS_LTM_score'] = df['quality_+_LTM_EPS_MTUM'] + df['value_+_LTM_EPS_MTUM']\n",
    "    df['CoE_EPS_LTM_Rank']  = df.groupby('Date')['CoE_EPS_LTM_score'].rank(ascending=True, method='min')\n",
    "    df['CoE_EPS_NTM_score'] = df['quality_+_NTM_EPS_MTUM'] + df['value_+_NTM_EPS_MTUM']\n",
    "    df['CoE_EPS_NTM_Rank']  = df.groupby('Date')['CoE_EPS_NTM_score'].rank(ascending=True, method='min')\n",
    "\n",
    "    df['EPS_TR_MTUM_score'] = df['EPS_NTM_Chg_SOM_Rank'] + df['TR_LTM_SOM_Rank']\n",
    "    df['EPS_TR_MTUM']       = df.groupby('Date')['EPS_TR_MTUM_score'].rank(ascending=True, method='min')\n",
    "\n",
    "    # -----------------------------\n",
    "    # 8) Portfolio building blocks\n",
    "    # -----------------------------\n",
    "    clean = (\n",
    "        df.copy()\n",
    "          .assign(Date=lambda x: pd.to_datetime(x['Date']))\n",
    "          .sort_values(['Date', 'ISIN'])\n",
    "          .dropna(subset=['TR', 'Mkt_Cap_SOM'])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    def index_monthly(frame):\n",
    "        m = frame.copy()\n",
    "        m['Cap_x_TR'] = m['TR'] * m['Mkt_Cap_SOM']\n",
    "        grp  = m.groupby(pd.Grouper(key='Date', freq='ME'))\n",
    "        sums = grp.agg(weighted_TR=(\"Cap_x_TR\", \"sum\"),\n",
    "                       mkt_sum     =(\"Mkt_Cap_SOM\", \"sum\"))\n",
    "        out = (sums['weighted_TR'] / sums['mkt_sum']).rename('Index_TR').to_frame()\n",
    "        out['Index_Net_NAV'] = (1 + out['Index_TR']).cumprod()\n",
    "        return out.reset_index()\n",
    "\n",
    "    def index_monthly(frame):\n",
    "        \"\"\"\n",
    "        Builds the index time series:\n",
    "          - Index_TR: cap-weighted average of monthly TR using Mkt_Cap_SOM\n",
    "          - Index_Net_NAV: cumprod(1 + Index_TR)\n",
    "          - IndexEqWgt_TR: equal-weight average of monthly TR across ISINs\n",
    "          - IndexEqWgt_Net_NAV: cumprod(1 + IndexEqWgt_TR)\n",
    "\n",
    "        Columns are ordered so the equal-weight series sit immediately to the right\n",
    "        of Index_TR and Index_Net_NAV in the Excel output.\n",
    "        \"\"\"\n",
    "        m = frame.copy()\n",
    "\n",
    "        # Cap-weighted monthly return\n",
    "        m['Cap_x_TR'] = m['TR'] * m['Mkt_Cap_SOM']\n",
    "        grp = m.groupby(pd.Grouper(key='Date', freq='ME'))\n",
    "\n",
    "        sums = grp.agg(\n",
    "            weighted_TR=(\"Cap_x_TR\", \"sum\"),\n",
    "            mkt_sum     =(\"Mkt_Cap_SOM\", \"sum\")\n",
    "        )\n",
    "        cap_tr = (sums['weighted_TR'] / sums['mkt_sum']).rename('Index_TR')\n",
    "\n",
    "        # Equal-weight monthly return (simple average of TR across names)\n",
    "        eq_tr = grp['TR'].mean().rename('IndexEqWgt_TR')\n",
    "\n",
    "        # Assemble and compute NAVs\n",
    "        out = pd.concat([cap_tr, eq_tr], axis=1).reset_index()\n",
    "        out['Index_Net_NAV']       = (1 + out['Index_TR']).cumprod()\n",
    "        out['IndexEqWgt_Net_NAV']  = (1 + out['IndexEqWgt_TR']).cumprod()\n",
    "\n",
    "        # Column order so they appear adjacent in the XLS\n",
    "        return out[['Date', 'Index_TR', 'IndexEqWgt_TR', 'Index_Net_NAV', 'IndexEqWgt_Net_NAV']]\n",
    "\n",
    "    # ---------- FIXED to coerce rank columns numeric before nsmallest/nlargest ----------\n",
    "    def ranked_factor_series(frame,\n",
    "                             *,\n",
    "                             rank_col: str,\n",
    "                             label:    str,\n",
    "                             keep_n:   int = keep_n,\n",
    "                             screen_n: int = screen_n,\n",
    "                             ret_col:  str = 'TR',\n",
    "                             ascending: bool = True):\n",
    "        \"\"\"\n",
    "        Builds monthly factor portfolio from a PRE-RANKED column.\n",
    "        Returns: nav_df (Date | <label>_NAV | <label>_Net_NAV),\n",
    "                 members_df (Date | Factor | ISIN | Weight)\n",
    "        \"\"\"\n",
    "        tmp = frame.copy()\n",
    "        # Ensure numeric before any sorting/selection\n",
    "        tmp[rank_col] = pd.to_numeric(tmp[rank_col], errors='coerce')\n",
    "        tmp[ret_col]  = pd.to_numeric(tmp[ret_col],  errors='coerce')\n",
    "        tmp = tmp.dropna(subset=[rank_col, ret_col])\n",
    "\n",
    "        tmp['Month'] = tmp['Date'].dt.to_period('M')\n",
    "\n",
    "        def pick_block(block: pd.DataFrame) -> pd.DataFrame:\n",
    "            b = block[['ISIN', rank_col, ret_col]].copy()\n",
    "            b[rank_col] = pd.to_numeric(b[rank_col], errors='coerce')\n",
    "            b[ret_col]  = pd.to_numeric(b[ret_col],  errors='coerce')\n",
    "            b = b.dropna(subset=[rank_col, ret_col])\n",
    "            if b.empty:\n",
    "                return b[['ISIN', ret_col]]\n",
    "            return (b.nsmallest(screen_n, rank_col) if ascending else b.nlargest(screen_n, rank_col))[['ISIN', ret_col]]\n",
    "\n",
    "        top_by_month = {m: pick_block(block) for m, block in tmp.groupby('Month')}\n",
    "\n",
    "        rows_nav, rows_members = [], []\n",
    "        prev = set()\n",
    "        for m in sorted(top_by_month):\n",
    "            cand = top_by_month[m]\n",
    "            if cand.empty:\n",
    "                continue\n",
    "            names  = list(cand['ISIN'])\n",
    "            keep_b = [n for n in names if n in prev]\n",
    "            add_b  = [n for n in names if n not in prev]\n",
    "            basket = (keep_b + add_b)[:keep_n]\n",
    "            gross_tr = cand.set_index('ISIN').loc[basket, ret_col].mean()\n",
    "            churn    = 1 - (len(keep_b) / keep_n) if prev else 1.0\n",
    "            net_tr   = gross_tr - TRADING_COST_RATE * churn\n",
    "            rows_nav.append({'Date': m.to_timestamp('M'), 'Gross_TR': gross_tr, 'Net_TR': net_tr})\n",
    "            w = 1.0 / keep_n if keep_n else np.nan\n",
    "            for s in basket:\n",
    "                rows_members.append({'Date': m.to_timestamp('M'), 'Factor': label, 'ISIN': s, 'Weight': w})\n",
    "            prev = set(basket)\n",
    "        nav = pd.DataFrame(rows_nav).sort_values('Date')\n",
    "        if not nav.empty:\n",
    "            nav[f'{label}_NAV']     = (1 + nav['Gross_TR']).cumprod()\n",
    "            nav[f'{label}_Net_NAV'] = (1 + nav['Net_TR']).cumprod()\n",
    "            nav = nav[['Date', f'{label}_NAV', f'{label}_Net_NAV']]\n",
    "        else:\n",
    "            nav = pd.DataFrame(columns=['Date', f'{label}_NAV', f'{label}_Net_NAV'])\n",
    "        return nav, pd.DataFrame(rows_members)\n",
    "\n",
    "    rank_cols_ready = [\n",
    "        # quality\n",
    "        'quality_+_LTM_Rank','quality_+_NTM_Rank','quality_+_LTM_TR_MTUM','quality_+_NTM_TR_MTUM','quality_+_LTM_EPS_MTUM','quality_+_NTM_EPS_MTUM',\n",
    "        # value\n",
    "        'value_+_LTM_Rank','value_+_NTM_Rank','value_+_LTM_TR_MTUM','value_+_NTM_TR_MTUM','value_+_LTM_EPS_MTUM','value_+_NTM_EPS_MTUM',\n",
    "        # CoE\n",
    "        'CoE_+_LTM_Rank','CoE_+_NTM_Rank','CoE_+_LTM_TR_MTUM','CoE_+_NTM_TR_MTUM','CoE_+_LTM_EPS_MTUM','CoE_+_NTM_EPS_MTUM',\n",
    "        # Other\n",
    "        'CoE_TR_LTM_Rank','CoE_TR_NTM_Rank','CoE_EPS_LTM_Rank','CoE_EPS_NTM_Rank','TR_LTM_SOM_Rank','EPS_NTM_Chg_SOM_Rank','EPS_TR_MTUM'\n",
    "    ]\n",
    "\n",
    "    nav_frames, members_frames = [], []\n",
    "    for col in rank_cols_ready:\n",
    "        label = col.replace('_Rank','')\n",
    "        nav_df, members_df = ranked_factor_series(clean, rank_col=col, label=label, ascending=True)\n",
    "        nav_frames.append(nav_df)\n",
    "        members_frames.append(members_df)\n",
    "\n",
    "    factors_df  = reduce(lambda l, r: l.merge(r, on='Date', how='left'), nav_frames)\n",
    "    members_all = pd.concat(members_frames, ignore_index=True)  # Date | Factor | ISIN | Weight\n",
    "\n",
    "    # Index + factors\n",
    "    index_df  = index_monthly(clean)\n",
    "    output_df = index_df.merge(factors_df, on='Date', how='left')\n",
    "\n",
    "    # -----------------------------\n",
    "    # 9) Optimiser: pick top fraction by 12m Net_NAV\n",
    "    # -----------------------------\n",
    "    net_nav_cols = [\n",
    "        c for c in output_df.columns\n",
    "        if c.endswith('Net_NAV') and c not in {'Index_Net_NAV','IndexEqWgt_Net_NAV'} ## strips out index measures from optimiser\n",
    "    ]\n",
    "    for col in net_nav_cols:\n",
    "        output_df[f'{col}_1m_return']  = output_df[col] / output_df[col].shift(1) - 1\n",
    "        output_df[f'{col}_12m_return'] = output_df[col].shift(1) / output_df[col].shift(13) - 1 # set this for the optimiser lookback period\n",
    "\n",
    "    def pick_top_fraction_factors(row):\n",
    "        twelve = row[[f'{c}_12m_return' for c in net_nav_cols]].dropna()\n",
    "        if twelve.empty:\n",
    "            return []\n",
    "        k = compute_top_k(len(twelve))  # uses FACTOR_TOP knob\n",
    "        chosen_idx = twelve.sort_values(ascending=False).index[:k]\n",
    "        return list(chosen_idx.str.replace('_12m_return', '', regex=False))\n",
    "\n",
    "    output_df['Selected_Factor_Cols'] = output_df.apply(pick_top_fraction_factors, axis=1)\n",
    "\n",
    "    def calc_filtered_net_tr(row):\n",
    "        chosen = row['Selected_Factor_Cols']\n",
    "        if not chosen:\n",
    "            return np.nan\n",
    "        one_m_cols = [f'{c}_1m_return' for c in chosen]\n",
    "        return row[one_m_cols].mean()\n",
    "\n",
    "    output_df['Filtered_Net_TR']      = output_df.apply(calc_filtered_net_tr, axis=1)\n",
    "    output_df['Filtered_Net_NAV']     = (1 + output_df['Filtered_Net_TR']).cumprod()\n",
    "    output_df['Filtered_Net_NAV_Rel'] = output_df['Filtered_Net_NAV'] / output_df['Index_Net_NAV']\n",
    "\n",
    "    # -----------------------------\n",
    "    # 10) Composite weights (SUM across factors) + turnover\n",
    "    # -----------------------------\n",
    "    sel = (\n",
    "        output_df[['Date', 'Selected_Factor_Cols']]\n",
    "          .dropna(subset=['Selected_Factor_Cols'])\n",
    "          .explode('Selected_Factor_Cols')\n",
    "          .rename(columns={'Selected_Factor_Cols':'FactorCol'})\n",
    "    )\n",
    "\n",
    "    # NaN-safe mapping of factor columns -> factor labels used in members_all\n",
    "    def col_to_factor_label(col):\n",
    "        if not isinstance(col, str):\n",
    "            return None\n",
    "        if col.endswith('_Net_NAV'):\n",
    "            return col[:-len('_Net_NAV')]\n",
    "        if col.endswith('_NAV'):\n",
    "            return col[:-len('_NAV')]\n",
    "        return col\n",
    "\n",
    "    sel['Factor'] = sel['FactorCol'].map(col_to_factor_label)\n",
    "    sel = sel.dropna(subset=['Factor'])\n",
    "\n",
    "    # Join to members to get constituents for the selected factors\n",
    "    selected_members = (\n",
    "        sel.merge(members_all, on=['Date','Factor'], how='left')\n",
    "          .dropna(subset=['ISIN'])\n",
    "    )\n",
    "\n",
    "    if selected_members.empty:\n",
    "        comp_weights = pd.DataFrame(columns=['Date','ISIN','Composite_Weight','Included_In_Factors'])\n",
    "        portfolio_view = comp_weights.copy()\n",
    "        portfolio_view['Weight_%'] = []\n",
    "        # Turnover scaffold\n",
    "        ts_dates = output_df[['Date']].drop_duplicates().sort_values('Date')\n",
    "        turnover = ts_dates.copy()\n",
    "        turnover['Gross_Turnover_%'] = 0.0\n",
    "        turnover = turnover.merge(output_df[['Date','Filtered_Net_NAV']].drop_duplicates(), on='Date', how='left')\n",
    "        turnover['Turnover_$'] = turnover['Gross_Turnover_%'] * turnover['Filtered_Net_NAV'].ffill().fillna(1.0)\n",
    "        turnover = turnover.drop(columns=['Filtered_Net_NAV']).set_index('Date')\n",
    "    else:\n",
    "        # SUM weights across selected factors (each factor contributes 1/KEEP_N per held stock)\n",
    "        comp_weights_raw = (\n",
    "            selected_members\n",
    "              .groupby(['Date','ISIN'], as_index=False)\n",
    "              .agg(\n",
    "                  SumWeight=('Weight','sum'),                   # proportional to #selected factors holding the ISIN\n",
    "                  Included_In_Factors=('Factor','nunique')\n",
    "              )\n",
    "        )\n",
    "\n",
    "        # Renormalize to sum to 1 within each month\n",
    "        comp_weights = (\n",
    "            comp_weights_raw\n",
    "              .merge(\n",
    "                  comp_weights_raw.groupby('Date', as_index=False)['SumWeight'].sum()\n",
    "                                  .rename(columns={'SumWeight':'_SumW'}),\n",
    "                  on='Date', how='left'\n",
    "              )\n",
    "        )\n",
    "        comp_weights['Composite_Weight'] = comp_weights['SumWeight'] / comp_weights['_SumW']\n",
    "        comp_weights = (comp_weights\n",
    "                        .drop(columns=['_SumW'])\n",
    "                        .sort_values(['Date','ISIN'])\n",
    "                        .reset_index(drop=True))\n",
    "\n",
    "        # Export view\n",
    "        portfolio_view = (\n",
    "            comp_weights[['Date','ISIN','Composite_Weight']]\n",
    "              .sort_values(['Date','ISIN'])\n",
    "              .assign(**{'Weight_%': lambda x: x['Composite_Weight'] * 100.0})\n",
    "        )\n",
    "\n",
    "        # Turnover (gross, from weight changes)\n",
    "        prev_w = comp_weights.pivot(index='Date', columns='ISIN', values='Composite_Weight').shift(1).fillna(0.0)\n",
    "        curr_w = comp_weights.pivot(index='Date', columns='ISIN', values='Composite_Weight').fillna(0.0)\n",
    "        prev_w, curr_w = prev_w.align(curr_w, join='outer', axis=None)\n",
    "        prev_w = prev_w.fillna(0.0); curr_w = curr_w.fillna(0.0)\n",
    "        trade_w = curr_w - prev_w\n",
    "\n",
    "        nav_series = output_df.set_index('Date')['Filtered_Net_NAV'].reindex(trade_w.index).ffill().fillna(1.0)\n",
    "        turnover = trade_w.abs().sum(axis=1).rename('Gross_Turnover_%').to_frame()\n",
    "        turnover['Turnover_$'] = turnover['Gross_Turnover_%'] * nav_series.values\n",
    "\n",
    "    # -----------------------------\n",
    "    # 10b) LIVE portfolio using the latest available data (build to trade next month)\n",
    "    # -----------------------------\n",
    "    # Determine the latest coherent month where all ingredients exist\n",
    "    signals_date = pd.to_datetime(df['Date']).max()                  # last month with signals\n",
    "    nav_date     = pd.to_datetime(output_df['Date']).max()           # last month with factor NAVs\n",
    "    today_live   = min(d for d in [signals_date, nav_date] if pd.notnull(d)) if (pd.notnull(signals_date) and pd.notnull(nav_date)) else pd.NaT\n",
    "\n",
    "    members_date = pd.to_datetime(members_all['Date']).max() if not members_all.empty else pd.NaT\n",
    "    prev_date    = members_date if pd.notnull(members_date) else today_live\n",
    "\n",
    "    live_selected_factors = pd.DataFrame(columns=['Date','Factor','T12M_Net_Return'])\n",
    "    live_factor_baskets   = pd.DataFrame(columns=['Date','Factor','ISIN','Weight'])\n",
    "    live_portfolio        = pd.DataFrame(columns=['Date','ISIN','Composite_Weight','Included_In_Factors','Weight_%'])\n",
    "\n",
    "    if pd.notnull(today_live):\n",
    "        TODAY = today_live\n",
    "\n",
    "        # (1) NOW frame at TODAY (non-SOM signals)\n",
    "        now_df = (\n",
    "            df[df['Date'] == TODAY]\n",
    "              .copy()\n",
    "              .dropna(subset=['ISIN'])\n",
    "        )\n",
    "\n",
    "        if not now_df.empty:\n",
    "            # medians for gating (non-SOM)\n",
    "            for col in ['EBITDA_Assets_LTM_5yAvg','CF_Assets_LTM_5yAvg','RoE_LTM_5yAvg']:\n",
    "                now_df[f\"{col}_Median_NOW\"] = now_df[col].median()\n",
    "\n",
    "            # gated + versions\n",
    "            now_df['EBITDA_Assets_LTM_+'] = np.where(now_df['EBITDA_Assets_LTM_5yAvg'] > now_df['EBITDA_Assets_LTM_5yAvg_Median_NOW'], now_df['EBITDA_Assets_LTM'], np.nan)\n",
    "            now_df['CF_Assets_LTM_+']     = np.where(now_df['CF_Assets_LTM_5yAvg']     > now_df['CF_Assets_LTM_5yAvg_Median_NOW'],     now_df['CF_Assets_LTM'],     np.nan)\n",
    "            now_df['RoE_LTM_+']           = np.where(now_df['RoE_LTM_5yAvg']           > now_df['RoE_LTM_5yAvg_Median_NOW'],           now_df['RoE_LTM'],           np.nan)\n",
    "\n",
    "            now_df['EBITDA_Assets_NTM_+'] = np.where(now_df['EBITDA_Assets_LTM_5yAvg'] > now_df['EBITDA_Assets_LTM_5yAvg_Median_NOW'], now_df['EBITDA_Assets_NTM'], np.nan)\n",
    "            now_df['CF_Assets_NTM_+']     = np.where(now_df['CF_Assets_LTM_5yAvg']     > now_df['CF_Assets_LTM_5yAvg_Median_NOW'],     now_df['CF_Assets_NTM'],     np.nan)\n",
    "            now_df['RoE_NTM_+']           = np.where(now_df['RoE_LTM_5yAvg']           > now_df['RoE_LTM_5yAvg_Median_NOW'],           now_df['RoE_NTM'],           np.nan)\n",
    "\n",
    "            # base NOW ranks for legs (no SOM)\n",
    "            base_now_cols = {\n",
    "                'TR_LTM':             'TR_LTM',\n",
    "                'EPS_NTM_Chg':        'EPS_NTM_Chg',\n",
    "                'EBITDA_Assets_LTM_+':'EBITDA_Assets_LTM_+',\n",
    "                'CF_Assets_LTM_+':    'CF_Assets_LTM_+',\n",
    "                'RoE_LTM_+':          'RoE_LTM_+',\n",
    "                'EBITDA_Assets_NTM_+':'EBITDA_Assets_NTM_+',\n",
    "                'CF_Assets_NTM_+':    'CF_Assets_NTM_+',\n",
    "                'RoE_NTM_+':          'RoE_NTM_+',\n",
    "                'Sales_EV_LTM':       'Sales_EV_LTM',\n",
    "                'EBITDA_EV_LTM':      'EBITDA_EV_LTM',\n",
    "                'EY_LTM':             'EY_LTM',\n",
    "                'CFY_LTM':            'CFY_LTM',\n",
    "                'DY_LTM':             'DY_LTM',\n",
    "                'Sales_EV_NTM':       'Sales_EV_NTM',\n",
    "                'EBITDA_EV_NTM':      'EBITDA_EV_NTM',\n",
    "                'EY_NTM':             'EY_NTM',\n",
    "                'CFY_NTM':            'CFY_NTM',\n",
    "                'DY_NTM':             'DY_NTM',\n",
    "            }\n",
    "            for lbl, col in base_now_cols.items():\n",
    "                # Coerce base columns to numeric before ranking to avoid object dtype\n",
    "                now_df[col] = pd.to_numeric(now_df[col], errors='coerce')\n",
    "                now_df[f'{lbl}_NOW_Rank'] = now_df[col].rank(ascending=False, method='min')\n",
    "\n",
    "            # robust helper\n",
    "            def robust_avg(row, cols, allow_k_drop=1):\n",
    "                n = len(cols)\n",
    "                # Coerce to numeric row-wise to avoid object dtype issues\n",
    "                vals = pd.to_numeric(row[cols], errors='coerce').dropna()\n",
    "                # If not enough valid inputs after coercion, return NaN\n",
    "                if len(vals) < max(1, n - allow_k_drop):\n",
    "                    return np.nan\n",
    "                k = max(1, n - allow_k_drop)\n",
    "                return vals.nsmallest(k).mean()\n",
    "\n",
    "            # composites NOW (mirror backtest logic) -> produce *_NOW_Rank directly\n",
    "            now_df['quality_+_LTM_NOW_Rank'] = now_df.apply(\n",
    "                lambda r: robust_avg(r, ['EBITDA_Assets_LTM_+_NOW_Rank','CF_Assets_LTM_+_NOW_Rank','RoE_LTM_+_NOW_Rank'], allow_k_drop=1), axis=1\n",
    "            )\n",
    "            now_df['quality_+_NTM_NOW_Rank'] = now_df.apply(\n",
    "                lambda r: robust_avg(r, ['EBITDA_Assets_NTM_+_NOW_Rank','CF_Assets_NTM_+_NOW_Rank','RoE_NTM_+_NOW_Rank'], allow_k_drop=1), axis=1\n",
    "            )\n",
    "\n",
    "            # value (best n-2 of 5)\n",
    "            value_ltm_now_cols = ['Sales_EV_LTM_NOW_Rank','EBITDA_EV_LTM_NOW_Rank','EY_LTM_NOW_Rank','CFY_LTM_NOW_Rank','DY_LTM_NOW_Rank']\n",
    "            value_ntm_now_cols = ['Sales_EV_NTM_NOW_Rank','EBITDA_EV_NTM_NOW_Rank','EY_NTM_NOW_Rank','CFY_NTM_NOW_Rank','DY_NTM_NOW_Rank']\n",
    "            now_df['value_+_LTM_NOW_Rank'] = now_df.apply(lambda r: robust_avg(r, value_ltm_now_cols, allow_k_drop=2), axis=1)\n",
    "            now_df['value_+_NTM_NOW_Rank'] = now_df.apply(lambda r: robust_avg(r, value_ntm_now_cols, allow_k_drop=2), axis=1)\n",
    "\n",
    "            # momentum blends & CoE\n",
    "            now_df['quality_+_LTM_TR_MTUM_NOW_Rank'] = (now_df['quality_+_LTM_NOW_Rank'] + now_df['TR_LTM_NOW_Rank']).rank(ascending=True, method='min')\n",
    "            now_df['quality_+_NTM_TR_MTUM_NOW_Rank'] = (now_df['quality_+_NTM_NOW_Rank'] + now_df['TR_LTM_NOW_Rank']).rank(ascending=True, method='min')\n",
    "            now_df['quality_+_LTM_EPS_MTUM_NOW_Rank'] = (now_df['quality_+_LTM_NOW_Rank'] + now_df['EPS_NTM_Chg_NOW_Rank']).rank(ascending=True, method='min')\n",
    "            now_df['quality_+_NTM_EPS_MTUM_NOW_Rank'] = (now_df['quality_+_NTM_NOW_Rank'] + now_df['EPS_NTM_Chg_NOW_Rank']).rank(ascending=True, method='min')\n",
    "\n",
    "            now_df['value_+_LTM_TR_MTUM_NOW_Rank'] = (now_df['value_+_LTM_NOW_Rank'] + now_df['TR_LTM_NOW_Rank']).rank(ascending=True, method='min')\n",
    "            now_df['value_+_NTM_TR_MTUM_NOW_Rank'] = (now_df['value_+_NTM_NOW_Rank'] + now_df['TR_LTM_NOW_Rank']).rank(ascending=True, method='min')\n",
    "            now_df['value_+_LTM_EPS_MTUM_NOW_Rank'] = (now_df['value_+_LTM_NOW_Rank'] + now_df['EPS_NTM_Chg_NOW_Rank']).rank(ascending=True, method='min')\n",
    "            now_df['value_+_NTM_EPS_MTUM_NOW_Rank'] = (now_df['value_+_NTM_NOW_Rank'] + now_df['EPS_NTM_Chg_NOW_Rank']).rank(ascending=True, method='min')\n",
    "\n",
    "            now_df['CoE_+_LTM_NOW_Rank'] = (now_df['quality_+_LTM_NOW_Rank'] + now_df['value_+_LTM_NOW_Rank']).rank(ascending=True, method='min')\n",
    "            now_df['CoE_+_NTM_NOW_Rank'] = (now_df['quality_+_NTM_NOW_Rank'] + now_df['value_+_NTM_NOW_Rank']).rank(ascending=True, method='min')\n",
    "\n",
    "            now_df['CoE_+_LTM_TR_MTUM_NOW_Rank'] = (now_df['CoE_+_LTM_NOW_Rank'] + now_df['TR_LTM_NOW_Rank']).rank(ascending=True, method='min')\n",
    "            now_df['CoE_+_NTM_TR_MTUM_NOW_Rank'] = (now_df['CoE_+_NTM_NOW_Rank'] + now_df['TR_LTM_NOW_Rank']).rank(ascending=True, method='min')\n",
    "            now_df['CoE_+_LTM_EPS_MTUM_NOW_Rank'] = (now_df['CoE_+_LTM_NOW_Rank'] + now_df['EPS_NTM_Chg_NOW_Rank']).rank(ascending=True, method='min')\n",
    "            now_df['CoE_+_NTM_EPS_MTUM_NOW_Rank'] = (now_df['CoE_+_NTM_NOW_Rank'] + now_df['EPS_NTM_Chg_NOW_Rank']).rank(ascending=True, method='min')\n",
    "\n",
    "            now_df['CoE_TR_LTM_NOW_Rank']  = (now_df['quality_+_LTM_TR_MTUM_NOW_Rank'] + now_df['value_+_LTM_TR_MTUM_NOW_Rank']).rank(ascending=True, method='min')\n",
    "            now_df['CoE_TR_NTM_NOW_Rank']  = (now_df['quality_+_NTM_TR_MTUM_NOW_Rank'] + now_df['value_+_NTM_TR_MTUM_NOW_Rank']).rank(ascending=True, method='min')\n",
    "            now_df['CoE_EPS_LTM_NOW_Rank'] = (now_df['quality_+_LTM_EPS_MTUM_NOW_Rank'] + now_df['value_+_LTM_EPS_MTUM_NOW_Rank']).rank(ascending=True, method='min')\n",
    "            now_df['CoE_EPS_NTM_NOW_Rank'] = (now_df['quality_+_NTM_EPS_MTUM_NOW_Rank'] + now_df['value_+_NTM_EPS_MTUM_NOW_Rank']).rank(ascending=True, method='min')\n",
    "\n",
    "            now_df['EPS_TR_MTUM_NOW_Rank'] = (now_df['EPS_NTM_Chg_NOW_Rank'] + now_df['TR_LTM_NOW_Rank']).rank(ascending=True, method='min')\n",
    "\n",
    "            # ---- (2) LIVE factor selection: top fraction by 12m Net_NAV through TODAY ----\n",
    "            last_row = output_df[output_df['Date'] == TODAY]\n",
    "            live_date_for_selection = TODAY if not last_row.empty else output_df['Date'].max()\n",
    "\n",
    "            live_sel_row = output_df.set_index('Date').sort_index()\n",
    "            live_sel = {}\n",
    "            for col in net_nav_cols:\n",
    "                nav_series = live_sel_row[col]\n",
    "                if live_date_for_selection in nav_series.index and pd.notnull(nav_series.loc[live_date_for_selection]) and pd.notnull(nav_series.shift(12).loc[live_date_for_selection]):\n",
    "                    live_sel[col] = nav_series.loc[live_date_for_selection] / nav_series.shift(12).loc[live_date_for_selection] - 1\n",
    "            live_sel = pd.Series(live_sel).dropna().sort_values(ascending=False)\n",
    "\n",
    "            k = compute_top_k(len(live_sel))  # knob-driven\n",
    "            chosen_factor_cols = list(live_sel.index[:k])  # e.g., 'quality_+_LTM_TR_MTUM_Net_NAV'\n",
    "            chosen_factors = [c.replace('_Net_NAV','') for c in chosen_factor_cols]\n",
    "\n",
    "            live_selected_factors = pd.DataFrame({\n",
    "                'Date': [live_date_for_selection]*len(chosen_factors),\n",
    "                'Factor': chosen_factors,\n",
    "                'T12M_Net_Return': [live_sel.get(f'{f}_Net_NAV', np.nan) for f in chosen_factors]\n",
    "            }).sort_values('T12M_Net_Return', ascending=False)\n",
    "\n",
    "            # ---- (3) Build next-month baskets using NOW ranks + previous month basket as keep-set ----\n",
    "            live_baskets = []\n",
    "            w_eq = 1.0 / keep_n if keep_n else np.nan\n",
    "\n",
    "            for factor in chosen_factors:\n",
    "                col_now_rank = f'{factor}_NOW_Rank'\n",
    "                if col_now_rank not in now_df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Coerce NOW rank column to numeric before nsmallest\n",
    "                candidates = (\n",
    "                    now_df[['ISIN', col_now_rank]]\n",
    "                      .assign(**{col_now_rank: lambda x: pd.to_numeric(x[col_now_rank], errors='coerce')})\n",
    "                      .dropna(subset=[col_now_rank])\n",
    "                      .nsmallest(screen_n, col_now_rank)\n",
    "                )\n",
    "                cand_isins = list(candidates['ISIN'])\n",
    "\n",
    "                prev_rows = members_all[(members_all['Date'] == prev_date) & (members_all['Factor'] == factor)]\n",
    "                prev_set = set(prev_rows['ISIN']) if not prev_rows.empty else set()\n",
    "\n",
    "                keep_bucket = [s for s in cand_isins if s in prev_set]\n",
    "                add_bucket  = [s for s in cand_isins if s not in prev_set]\n",
    "                basket = (keep_bucket + add_bucket)[:keep_n]\n",
    "\n",
    "                for s in basket:\n",
    "                    live_baskets.append({'Date': TODAY, 'Factor': factor, 'ISIN': s, 'Weight': w_eq})\n",
    "\n",
    "            live_factor_baskets = pd.DataFrame(live_baskets).sort_values(['Factor','ISIN'])\n",
    "\n",
    "            # ---- (4) Aggregate across factors to composite LIVE portfolio ----\n",
    "            if not live_factor_baskets.empty:\n",
    "                comp_raw = (\n",
    "                    live_factor_baskets\n",
    "                        .groupby(['Date','ISIN'], as_index=False)\n",
    "                        .agg(SumWeight=('Weight','sum'), Included_In_Factors=('Factor','nunique'))\n",
    "                )\n",
    "                total = comp_raw.groupby('Date', as_index=False)['SumWeight'].sum().rename(columns={'SumWeight':'_SumW'})\n",
    "                comp = comp_raw.merge(total, on='Date', how='left')\n",
    "                comp['Composite_Weight'] = comp['SumWeight'] / comp['_SumW']\n",
    "                live_portfolio = (\n",
    "                    comp.drop(columns=['_SumW'])\n",
    "                        .assign(**{'Weight_%': lambda x: x['Composite_Weight']*100.0})\n",
    "                        .sort_values(['ISIN'])\n",
    "                )\n",
    "\n",
    "            if pd.notnull(prev_date) and prev_date != TODAY:\n",
    "                print(f\"[{UNIVERSE}] Note: members_all ends at {prev_date.date()}, NOW signals/NAV use {TODAY.date()}.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 11) Export (lean) + LIVE sheets\n",
    "    # -----------------------------\n",
    "    with pd.ExcelWriter(export_xls, engine='openpyxl', datetime_format='yyyy-mm-dd') as writer:\n",
    "        # (1) backtest time series and helpers\n",
    "        output_df.to_excel(writer, sheet_name='Filtered_NAV_TS', index=False)\n",
    "        # (2) monthly composite holdings (backtest)\n",
    "        portfolio_view.to_excel(writer, sheet_name='Portfolio_Constituents', index=False)\n",
    "        # (3) turnover (backtest)\n",
    "        turnover.reset_index().to_excel(writer, sheet_name='Composite_Turnover', index=False)\n",
    "        # (4) LIVE as-of latest month available (for next month trading)\n",
    "        live_selected_factors.to_excel(writer, sheet_name='Live_Selected_Factors', index=False)\n",
    "        live_factor_baskets.to_excel(writer, sheet_name='Live_Factor_Baskets', index=False)\n",
    "        live_portfolio.to_excel(writer, sheet_name='Live_Portfolio_Constituents', index=False)\n",
    "\n",
    "    print(f\"[{UNIVERSE}] Excel export written to: {export_xls}\")\n",
    "\n",
    "# ======================================\n",
    "# 6) Run the requested universes\n",
    "# ======================================\n",
    "for U in UNIVERSES:\n",
    "    run_universe(U, keep_n=KEEP_N, screen_n=SCREEN_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3000a9-6af7-4bde-b85f-d4c10566a136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— AUDIT SUMMARY —\n",
      "Universe: STOXX600 | As-of: 2025-08-31 | Column: TR\n",
      "Constituents (total/with-data/missing): 601/600/1\n",
      "Sum of cap weights: 1.0000000000  (error: -0.00 bp)\n",
      "Cap-weighted TR: 0.730829\n",
      "Equal-weight TR: 0.076439\n",
      "Bounds OK? cap=True  eq=True\n",
      "\n",
      "Top 10 contributors:\n",
      "        ISIN    w_cap       val  contribution\n",
      "GB00BP6MXD84 0.013024 60.493199      0.787849\n",
      "FI4000297767 0.003137  1.253542      0.003932\n",
      "NL0011585146 0.005010  0.311591      0.001561\n",
      "DK0062498333 0.010106  0.128385      0.001297\n",
      "NL0010273215 0.017079  0.061164      0.001045\n",
      "CH0012005267 0.015142  0.066664      0.001009\n",
      "GB0009895292 0.014361  0.056101      0.000806\n",
      "GB0005405286 0.013147  0.049485      0.000651\n",
      "FR0000120578 0.006860  0.094369      0.000647\n",
      "ES0113900J37 0.007934  0.081345      0.000645\n",
      "\n",
      "Bottom 10 contributors:\n",
      "        ISIN    w_cap       val  contribution\n",
      "DE0007164600 0.021836 -0.950143     -0.020748\n",
      "DE0007236101 0.012701 -0.791306     -0.010050\n",
      "DE0008404005 0.009505 -0.937515     -0.008911\n",
      "DE0008430026 0.005330 -0.972467     -0.005183\n",
      "DE0007030009 0.005627 -0.747072     -0.004204\n",
      "DE0005140008 0.003972 -0.772065     -0.003067\n",
      "DE0005190003 0.003318 -0.873951     -0.002900\n",
      "DE000ENAG999 0.002977 -0.836470     -0.002490\n",
      "DE000BASF111 0.002715 -0.711714     -0.001932\n",
      "DE0006047004 0.002556 -0.688844     -0.001760\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# AUDIT: point-in-time check\n",
    "# =========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- user knobs ---\n",
    "AUDIT_UNIVERSE = \"STOXX600\"         # e.g., \"STOXX600\", \"FTSE250\", \"SnP500\"\n",
    "AUDIT_DATE_STR = \"31 Aug 2025\"      # any parseable date; will be aligned to month-end\n",
    "AUDIT_COL      = \"TR\"               # e.g., \"TR\", \"TR_LTM_SOM\", \"EY_LTM_SOM\", etc.\n",
    "\n",
    "# --- reuse your base paths defined above ---\n",
    "# BASE, CONSTIT_DIR, CSV_DIR, SPOT_DIR must already exist in your session.\n",
    "\n",
    "def _month_end(ts_like) -> pd.Timestamp:\n",
    "    d = pd.to_datetime(ts_like, dayfirst=True, errors='coerce')\n",
    "    if pd.isna(d):\n",
    "        raise ValueError(f\"Cannot parse date: {ts_like}\")\n",
    "    return (d + pd.offsets.MonthEnd(0)).normalize()\n",
    "\n",
    "ASOF = _month_end(AUDIT_DATE_STR)\n",
    "\n",
    "def _load_constituents(universe: str) -> pd.DataFrame:\n",
    "    p = os.path.join(CONSTIT_DIR, f\"{universe}_Constit.csv\")\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Constituents file not found: {p}\")\n",
    "    dfc = pd.read_csv(p, header=0)\n",
    "    # columns are MMM-YY like '31-Aug-25' per your pipeline\n",
    "    dfc.columns = pd.to_datetime(dfc.columns, format='%d-%b-%y')\n",
    "    dfc = pd.melt(dfc, id_vars=[], var_name='Date', value_name='ISIN')\n",
    "    dfc['Date'] = dfc['Date'] + pd.offsets.MonthEnd(0)\n",
    "    dfc['ISIN'] = dfc['ISIN'].fillna('placeholder')\n",
    "    return dfc\n",
    "\n",
    "def _load_spot(universe: str) -> pd.DataFrame:\n",
    "    p = os.path.join(SPOT_DIR, f\"{universe}_Spot.csv\")\n",
    "    if not os.path.exists(p):\n",
    "        return pd.DataFrame(columns=['ISIN','Date'])\n",
    "    s = pd.read_csv(p)\n",
    "    s.columns = [c.strip() for c in s.columns]\n",
    "    if 'ISIN' not in s.columns or 'Date' not in s.columns:\n",
    "        raise ValueError(f\"{universe}_Spot.csv must include 'ISIN' and 'Date'\")\n",
    "    # use your robust parser if present; otherwise safe parse:\n",
    "    try:\n",
    "        s['Date'] = parse_date_column(s['Date'])\n",
    "    except NameError:\n",
    "        s['Date'] = pd.to_datetime(s['Date'], errors='coerce', dayfirst=True)\n",
    "    s = s.dropna(subset=['ISIN','Date']).assign(ISIN=lambda x: x['ISIN'].astype(str).str.strip())\n",
    "    return s\n",
    "\n",
    "def _load_hist_for_isin(isin: str, spot_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    p = os.path.join(CSV_DIR, f\"{isin}.csv\")\n",
    "    if not os.path.exists(p):\n",
    "        return pd.DataFrame()\n",
    "    h = pd.read_csv(p, header=0, na_values=[\"-\"])\n",
    "    # Excel serials -> datetime (your convention)\n",
    "    h['Date'] = pd.to_datetime(h['Date'], unit='D', origin='1899-12-30', errors='coerce')\n",
    "    # add ISIN column if missing\n",
    "    if 'ISIN' not in h.columns:\n",
    "        h['ISIN'] = isin\n",
    "    else:\n",
    "        h['ISIN'] = h['ISIN'].fillna(isin).astype(str).str.strip()\n",
    "\n",
    "    # append matching spot rows\n",
    "    if not spot_df.empty:\n",
    "        add = spot_df[spot_df['ISIN'] == isin].copy()\n",
    "        if not add.empty:\n",
    "            add['Date'] = pd.to_datetime(add['Date'], errors='coerce')\n",
    "            add['ISIN'] = isin\n",
    "            h = (pd.concat([h, add], ignore_index=True, sort=False)\n",
    "                   .sort_values('Date')\n",
    "                   .drop_duplicates(subset=['Date'], keep='last')\n",
    "                   .reset_index(drop=True))\n",
    "    # align to month-end\n",
    "    h['Date'] = h['Date'] + pd.offsets.MonthEnd(0)\n",
    "\n",
    "    # numeric coercions for the minimum needed fields\n",
    "    for col in ['Mkt_Cap','RI','P','Rev_LTM','Rev_NTM','BPS_LTM','BPS_NTM','EPS_LTM','EPS_NTM',\n",
    "                'EBITDA_LTM_DS','EBITDA_LTM','EBITDA_NTM','NetDebt_LTM_DS','NetDebt_LTM','NetDebt_NTM']:\n",
    "        if col not in h.columns: h[col] = np.nan\n",
    "    for col in h.columns:\n",
    "        if col not in ['Date','ISIN']:\n",
    "            h[col] = pd.to_numeric(h[col], errors='coerce')\n",
    "\n",
    "    # compute TR per your logic (from RI)\n",
    "    h['RI'] = np.where(h['RI'] == 0, np.nan, h['RI'])\n",
    "    h['TR'] = h['RI'] / h['RI'].shift(1) - 1\n",
    "\n",
    "    # SOM versions\n",
    "    h['Mkt_Cap_SOM'] = h['Mkt_Cap'].shift(1)\n",
    "\n",
    "    return h[['Date','ISIN','Mkt_Cap_SOM','TR']].copy()\n",
    "\n",
    "def build_audit_frame(universe: str) -> pd.DataFrame:\n",
    "    cons = _load_constituents(universe)\n",
    "    spot = _load_spot(universe)\n",
    "    all_frames = []\n",
    "    for isin in cons['ISIN'].unique():\n",
    "        if isin == 'placeholder':\n",
    "            continue\n",
    "        dat = _load_hist_for_isin(isin, spot)\n",
    "        if not dat.empty:\n",
    "            all_frames.append(dat)\n",
    "    if not all_frames:\n",
    "        raise RuntimeError(f\"No ISIN histories found under {CSV_DIR} for {universe}\")\n",
    "    panel = (pd.concat(all_frames, ignore_index=True)\n",
    "               .dropna(subset=['Date','ISIN'])\n",
    "               .sort_values(['Date','ISIN']))\n",
    "    return panel\n",
    "\n",
    "def audit_point_in_time(universe: str, asof: pd.Timestamp, col: str = 'TR') -> dict:\n",
    "    panel = build_audit_frame(universe)\n",
    "    # filter to the month-end slice\n",
    "    snap = panel[panel['Date'] == asof].copy()\n",
    "    # ensure column exists\n",
    "    if col not in snap.columns:\n",
    "        # if user asked for a SOM/other col that isn't in the minimal load, warn\n",
    "        available = list(snap.columns)\n",
    "        raise KeyError(f\"Requested column '{col}' not in snapshot. Available: {available}\")\n",
    "    # base filters for a fair comparison\n",
    "    snap['val'] = pd.to_numeric(snap[col], errors='coerce')\n",
    "    base = snap.dropna(subset=['Mkt_Cap_SOM','val']).copy()\n",
    "\n",
    "    stats = {}\n",
    "    stats['asof'] = asof.date()\n",
    "    stats['universe'] = universe\n",
    "    stats['metric_col'] = col\n",
    "    stats['n_constituents_total'] = int(snap['ISIN'].nunique())\n",
    "    stats['n_with_data'] = int(base['ISIN'].nunique())\n",
    "    stats['n_missing_val'] = int(snap['ISIN'].nunique() - base['ISIN'].nunique())\n",
    "\n",
    "    if base.empty:\n",
    "        stats['note'] = \"No valid rows with both Mkt_Cap_SOM and metric.\"\n",
    "        return {'stats': stats, 'weights_ok': False, 'cap_weighted': np.nan,\n",
    "                'equal_weighted': np.nan, 'top_contrib': pd.DataFrame(), 'bottom_contrib': pd.DataFrame()}\n",
    "\n",
    "    # weights\n",
    "    base['w_cap'] = base['Mkt_Cap_SOM'] / base['Mkt_Cap_SOM'].sum()\n",
    "    wsum = float(base['w_cap'].sum())\n",
    "    stats['weight_sum'] = wsum\n",
    "    stats['weight_sum_error_bp'] = (wsum - 1.0) * 1e4\n",
    "\n",
    "    # aggregates\n",
    "    cap_weighted = float((base['val'] * base['w_cap']).sum())\n",
    "    equal_weighted = float(base['val'].mean())\n",
    "    stats['cap_weighted'] = cap_weighted\n",
    "    stats['equal_weighted'] = equal_weighted\n",
    "\n",
    "    # contributions\n",
    "    base['contribution'] = base['val'] * base['w_cap']\n",
    "    top = (base[['ISIN','w_cap','val','contribution']]\n",
    "           .sort_values('contribution', ascending=False)\n",
    "           .head(10)\n",
    "           .reset_index(drop=True))\n",
    "    bot = (base[['ISIN','w_cap','val','contribution']]\n",
    "           .sort_values('contribution', ascending=True)\n",
    "           .head(10)\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "    # reasonableness bands for 'return-like' columns\n",
    "    if 'TR' in col.upper():\n",
    "        stats['cap_weighted_bounds_ok'] = (-0.95 <= cap_weighted <= 5.0)  # generous guardrails\n",
    "        stats['equal_weighted_bounds_ok'] = (-0.95 <= equal_weighted <= 5.0)\n",
    "    else:\n",
    "        stats['cap_weighted_bounds_ok'] = None\n",
    "        stats['equal_weighted_bounds_ok'] = None\n",
    "\n",
    "    weights_ok = abs(wsum - 1.0) < 1e-8\n",
    "\n",
    "    # print a tight summary\n",
    "    print(\"— AUDIT SUMMARY —\")\n",
    "    print(f\"Universe: {universe} | As-of: {asof.date()} | Column: {col}\")\n",
    "    print(f\"Constituents (total/with-data/missing): {stats['n_constituents_total']}/{stats['n_with_data']}/{stats['n_missing_val']}\")\n",
    "    print(f\"Sum of cap weights: {wsum:.10f}  (error: {stats['weight_sum_error_bp']:.2f} bp)\")\n",
    "    print(f\"Cap-weighted {col}: {cap_weighted:.6f}\")\n",
    "    print(f\"Equal-weight {col}: {equal_weighted:.6f}\")\n",
    "    if 'TR' in col.upper():\n",
    "        print(f\"Bounds OK? cap={stats['cap_weighted_bounds_ok']}  eq={stats['equal_weighted_bounds_ok']}\")\n",
    "    print(\"\\nTop 10 contributors:\")\n",
    "    print(top.to_string(index=False))\n",
    "    print(\"\\nBottom 10 contributors:\")\n",
    "    print(bot.to_string(index=False))\n",
    "\n",
    "    return {\n",
    "        'stats': stats,\n",
    "        'weights_ok': weights_ok,\n",
    "        'cap_weighted': cap_weighted,\n",
    "        'equal_weighted': equal_weighted,\n",
    "        'top_contrib': top,\n",
    "        'bottom_contrib': bot\n",
    "    }\n",
    "\n",
    "# ---- run the audit ----\n",
    "_ = audit_point_in_time(AUDIT_UNIVERSE, ASOF, AUDIT_COL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
